[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MINI Project 00",
    "section": "",
    "text": "** Maria Alexandra Jerez**"
  },
  {
    "objectID": "mp01.html",
    "href": "mp01.html",
    "title": "Mini-Project #01: Fiscal Characteristics of Major US Public Transit Systems",
    "section": "",
    "text": "```{r}\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\n# Let's start with Fare Revenue\nlibrary(tidyverse)\nif(!file.exists(\"2022_fare_revenue.xlsx\")){\n# This should work _in theory_ but in practice it's still a bit finicky\n# If it doesn't work for you, download this file 'by hand' in your\n# browser and save it as \"2022_fare_revenue.xlsx\" in your project\n# directory.\ndownload.file(\"http://www.transit.dot.gov/sites/fta.dot.gov/files/2024-04/2022%20Fare%20Revenue.xlsx\",\ndestfile=\"2022_fare_revenue.xlsx\",\nquiet=FALSE,\nmethod=\"wget\")\n}\nFARES &lt;- readxl::read_xlsx(\"2022_fare_revenue.xlsx\") |&gt;\nselect(-`State/Parent NTD ID`,\n-`Reporter Type`,\n-`Reporting Module`,\n-`TOS`,\n-`Passenger Paid Fares`,\n-`Organization Paid Fares`) |&gt;\nfilter(`Expense Type` == \"Funds Earned During Period\") |&gt;\nselect(-`Expense Type`) |&gt;\ngroup_by(`NTD ID`,       # Sum over different `TOS` for the same `Mode`\n`Agency Name`,  # These are direct operated and sub-contracted\n`Mode`) |&gt;      # of the same transit modality\n# Not a big effect in most munis (significant DO\n# tends to get rid of sub-contractors), but we'll sum\n# to unify different passenger experiences\nsummarize(`Total Fares` = sum(`Total Fares`)) |&gt;\nungroup()\n# Next, expenses\nif(!file.exists(\"2022_expenses.csv\")){\n# This should work _in theory_ but in practice it's still a bit finicky\n# If it doesn't work for you, download this file 'by hand' in your\n# browser and save it as \"2022_expenses.csv\" in your project\n# directory.\ndownload.file(\"https://data.transportation.gov/api/views/dkxx-zjd6/rows.csv?date=20231102&accessType=DOWNLOAD&bom=true&format=true\",\ndestfile=\"2022_expenses.csv\",\nquiet=FALSE,\nmethod=\"wget\")\n}\nEXPENSES &lt;- readr::read_csv(\"2022_expenses.csv\") |&gt;\nselect(`NTD ID`,\n`Agency`,\n`Total`,\n`Mode`) |&gt;\nmutate(`NTD ID` = as.integer(`NTD ID`)) |&gt;\nrename(Expenses = Total) |&gt;\ngroup_by(`NTD ID`, `Mode`) |&gt;\nsummarize(Expenses = sum(Expenses)) |&gt;\nungroup()\nFINANCIALS &lt;- inner_join(FARES, EXPENSES, join_by(`NTD ID`, `Mode`))\n# Monthly Transit Numbers\nlibrary(tidyverse)\nif(!file.exists(\"ridership.xlsx\")){\n# This should work _in theory_ but in practice it's still a bit finicky\n# If it doesn't work for you, download this file 'by hand' in your\n# browser and save it as \"ridership.xlsx\" in your project\n# directory.\ndownload.file(\"https://www.transit.dot.gov/sites/fta.dot.gov/files/2024-09/July%202024%20Complete%20Monthly%20Ridership%20%28with%20adjustments%20and%20estimates%29_240903.xlsx\",\ndestfile=\"ridership.xlsx\",\nquiet=FALSE,\nmethod=\"wget\")\n}\nTRIPS &lt;- readxl::read_xlsx(\"ridership.xlsx\", sheet=\"UPT\") |&gt;\nfilter(`Mode/Type of Service Status` == \"Active\") |&gt;\nselect(-`Legacy NTD ID`,\n-`Reporter Type`,\n-`Mode/Type of Service Status`,\n-`UACE CD`,\n-`TOS`) |&gt;\npivot_longer(-c(`NTD ID`:`3 Mode`),\nnames_to=\"month\",\nvalues_to=\"UPT\") |&gt;\ndrop_na() |&gt;\nmutate(month=my(month)) # Parse _m_onth _y_ear date specs\nMILES &lt;- readxl::read_xlsx(\"ridership.xlsx\", sheet=\"VRM\") |&gt;\nfilter(`Mode/Type of Service Status` == \"Active\") |&gt;\nselect(-`Legacy NTD ID`,\n-`Reporter Type`,\n-`Mode/Type of Service Status`,\n-`UACE CD`,\n-`TOS`) |&gt;\npivot_longer(-c(`NTD ID`:`3 Mode`),\nnames_to=\"month\",\nvalues_to=\"VRM\") |&gt;\ndrop_na() |&gt;\ngroup_by(`NTD ID`, `Agency`, `UZA Name`,\n`Mode`, `3 Mode`, month) |&gt;\nsummarize(VRM = sum(VRM)) |&gt;\nungroup() |&gt;\nmutate(month=my(month)) # Parse _m_onth _y_ear date specs\nUSAGE &lt;- inner_join(TRIPS, MILES) |&gt;\nmutate(`NTD ID` = as.integer(`NTD ID`))\ndistinct(USAGE)\nUSAGE &lt;- USAGE |&gt;\nmutate(Mode=case_when(\nMode == \"DR\" ~ \"Demand Response\",\nTRUE ~ \"Unknown\"))\nrename(USAGE, c(\"metro_area\" = \"UZA Name\", \"Unlinked Passenger Trips\"=\"UPT\",\"Vehicle Revenue Miles\"=\"VRM\"))\nif(!require(\"DT\")) install.packages(\"DT\")\nlibrary(DT)\nsample_n(USAGE, 1000) |&gt;\nmutate(month=as.character(month)) |&gt;\nDT::datatable()\n```\n\nWhat transit agency had the most total VRM in our data set?\n\n\n\nMTA New York City Transit\n10832855350\n\n\n\nWhat transit mode had the most total VRM in our data set?\n\n\n\nUnknown\n79125728755\n\n\n\nHow many trips were taken on the NYC Subway (Heavy Rail) in May 2024?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n20008\nMTA New York City Transit\nNew York–Jersey City–Newark, NY–NJ\nUnknown\nRail\n2024-05-01\n180458819\n30042876\n3 Mode"
  },
  {
    "objectID": "mp02.html",
    "href": "mp02.html",
    "title": "Mini-Project #02:The Business of Show Business",
    "section": "",
    "text": "Flower\n\n\n\nlibrary(tidyverse) \nlibrary(ggplot2)\n\n#| label: 'imdb_name_basics' \n#| message: false \n#| warning: false \n#| cache: true\n\nget_imdb_file &lt;- function(fname){ \n  BASE_URL &lt;- \"https://datasets.imdbws.com/\" \n  fname_ext &lt;- paste0(fname, \".tsv.gz\") \n  if(!file.exists(fname_ext)){ \n    FILE_URL &lt;- paste0(BASE_URL, fname_ext) \n    download.file(FILE_URL, \n                  destfile = fname_ext) \n} \nas.data.frame(readr::read_tsv(fname_ext, lazy=FALSE)) \n}\n\nNAME_BASICS &lt;- get_imdb_file(\"name.basics\")\n\nRows: 13886887 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (6): nconst, primaryName, birthYear, deathYear, primaryProfession, known...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#| label: 'imdb_title_basics' \n#| message: false \n#| warning: false \n#| cache: true \nTITLE_BASICS &lt;- get_imdb_file(\"title.basics\")\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 11176312 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (8): tconst, titleType, primaryTitle, originalTitle, startYear, endYear,...\ndbl (1): isAdult\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#| label: 'imdb_title_episode' \n#| message: false \n#| warning: false \n#| cache: true \n\nTITLE_EPISODES &lt;- get_imdb_file(\"title.episode\")\n\nRows: 8580365 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (4): tconst, parentTconst, seasonNumber, episodeNumber\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#| label: 'imdb_title_ratings' \n#| message: false \n#| warning: false \n#| cache: true \n\nTITLE_RATINGS &lt;- get_imdb_file(\"title.ratings\")\n\nRows: 1489867 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (1): tconst\ndbl (2): averageRating, numVotes\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#| label: 'imdb_title_crew' \n#| message: false \n#| warning: false \n#| cache: true \n\nTITLE_CREW &lt;- get_imdb_file(\"title.crew\")\n\nRows: 10515996 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (3): tconst, directors, writers\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#| label: 'imdb_title_principals' \n#| eval: false \n#| message: false \n#| warning: false \n#| cache: false \n\nTITLE_PRINCIPALS &lt;- get_imdb_file(\"title.principals\")\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 86651930 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (5): tconst, nconst, category, job, characters\ndbl (1): ordering\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#| label: 'name_basics_filter' \n#| cache: true \n \nNAME_BASICS &lt;- NAME_BASICS |&gt; \n  filter(str_count(knownForTitles, \",\") &gt; 1)\n\n#| label: 'title_ratings_tail' \n\nTITLE_RATINGS |&gt; ggplot(aes(x=numVotes)) + \n  geom_histogram(bins=30) + \n  xlab(\"Number of IMDB Ratings\") + \n  ylab(\"Number of Titles\") + \n  ggtitle(\"Majority of IMDB Titles Have Less than 100 Ratings\") + \n  theme_bw() + \n  scale_x_log10(label=scales::comma) + \n  scale_y_continuous(label=scales::comma)\n\n\n\n\n\n\n\n#| label: 'title_ratings_quantile' \n\nTITLE_RATINGS |&gt; pull(numVotes) |&gt; \n  quantile()\n\n     0%     25%     50%     75%    100% \n      5      11      26     100 2953131 \n\n#| label: 'title_ratings_filter' \n#| cache: true \n \nTITLE_RATINGS &lt;- TITLE_RATINGS |&gt; \n  filter(numVotes &gt;= 100)\n\n#| cache: true \n#| label: 'title_other_filter' \n#| message: false \n \nTITLE_BASICS &lt;- TITLE_BASICS |&gt; semi_join(TITLE_RATINGS, join_by(tconst == tconst))\n\nTITLE_CREW &lt;- TITLE_CREW |&gt; semi_join(TITLE_RATINGS, join_by(tconst == tconst))\n\nTITLE_EPISODES_1 &lt;- TITLE_EPISODES |&gt; semi_join(TITLE_RATINGS, join_by(tconst == tconst)) \n\nTITLE_EPISODES_2 &lt;- TITLE_EPISODES |&gt; semi_join(TITLE_RATINGS, join_by(parentTconst == tconst))\n\nTITLE_EPISODES &lt;- bind_rows(TITLE_EPISODES_1, TITLE_EPISODES_2) |&gt; distinct()\n\nTITLE_PRINCIPALS &lt;- TITLE_PRINCIPALS |&gt; semi_join(TITLE_RATINGS, join_by(tconst == tconst))\n\nrm(TITLE_EPISODES_1) \nrm(TITLE_EPISODES_2)\n\nNAME_BASICS &lt;- NAME_BASICS |&gt; \n  mutate(birthYear = as.numeric(birthYear), deathYear = as.numeric(deathYear))\n\nWarning: There were 2 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `birthYear = as.numeric(birthYear)`.\nCaused by warning:\n! NAs introduced by coercion\nℹ Run `dplyr::last_dplyr_warnings()` to see the 1 remaining warning.\n\n\n\n\n\nTITLE_BASICS &lt;- TITLE_BASICS |&gt; \n  mutate(startYear = as.numeric(startYear), runtimeMinutes = as.numeric(runtimeMinutes), endYear = as.numeric(endYear))\n\nWarning: There were 3 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `startYear = as.numeric(startYear)`.\nCaused by warning:\n! NAs introduced by coercion\nℹ Run `dplyr::last_dplyr_warnings()` to see the 2 remaining warnings.\n\nTITLE_CREW &lt;- TITLE_CREW |&gt; \n  mutate(writers = as.logical(writers))\n\nTITLE_EPISODES &lt;- TITLE_EPISODES |&gt; \n  mutate(seasonNumber = as.numeric(seasonNumber), episodeNumber = as.numeric(episodeNumber))\n\nWarning: There were 2 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `seasonNumber = as.numeric(seasonNumber)`.\nCaused by warning:\n! NAs introduced by coercion\nℹ Run `dplyr::last_dplyr_warnings()` to see the 1 remaining warning.\n\nTITLE_PRINCIPALS[TITLE_PRINCIPALS == \"\\\\N\"] &lt;- NA\n\n\n\n\nHow many movies are in our data set? How many TV series? How many TV episodes?\nANSWER : MOVIES TOTAL = 118546, TV series = 28745 , tv episodes = 121731\n\ndf = TITLE_BASICS %&gt;% group_by(titleType, primaryTitle) %&gt;% \n    summarize(movie_total = sum(str_count(titleType, \"movie\")), \n            tvepisode_total = sum(str_count(titleType, \"tvEpisode\")), \n            tvSeries_total = sum(str_count(titleType, \"tvSeries\"))) \n\n`summarise()` has grouped output by 'titleType'. You can override using the\n`.groups` argument.\n\na = filter(df,titleType == \"movie\") \nb = filter(df,titleType == \"tvEpisode\") \nc = filter(df,titleType == \"tvSeries\")\n\nWho is the oldest living person in our data set?\nNeed name, birth year = oldest and deathyear= NA\nANSWER : Assuming the oldest person alive is represented by deathYear = NA, for the oldest person alive greater than 1917 [oldest person alive] ,from our data set, there are 89 people still alive\n\noldest_person &lt;- NAME_BASICS |&gt; \n  filter(birthYear &gt; 1917, is.na(deathYear)) |&gt; \n  arrange(birthYear) |&gt; \n  slice_head(n=89) #There are 89 names born on 1918, deathYear= NA\n\nThere is one TV Episode in this data set with a perfect 10/10 rating and at least 200,000 IMDb ratings. What is it? What series does it belong to?\nI Joined tilebasics where titletype = ‘tvepisode’ to titleratings using tconst, filter average rating = 10 ANSWER : The TV episode with perfect 10 rating, at 230087 votes, is Ozymandias at seson 5, episode 14\n\nrating &lt;- TITLE_RATINGS |&gt; \n  filter(averageRating == 10, numVotes &gt;= 200000) #This gives me tconst where rating =10 + &gt;200000\n\ntitle_a &lt;- TITLE_EPISODES |&gt; \n  filter(tconst == \"tt2301451\") # identify for unique episode\n\ntitle_b &lt;- TITLE_BASICS |&gt; \n  filter(tconst == \"tt2301451\") # identify for unique episode\n\nd = rating %&gt;% left_join(title_a, by = \"tconst\") #join for full view\n\ne = d %&gt;% left_join(title_b, by = \"tconst\") #further join for full view\n\nWhat four projects is the actor Mark Hamill most known for?\nI used name basics to filter to actor Mark Hamil , used titlebasics to get tittle of projects\nANSWER: Actor Hamil is known for Castlevania: Nocturne [tvSeries], Drifting Home [movie], Boruto: Naruto Next Generations [ tvSeries]\n\nactor_Hamil &lt;- NAME_BASICS |&gt; \n  filter(primaryName == \"Hamil\") |&gt; \n  separate_longer_delim(knownForTitles, \",\")\n\ntitle_Hamil &lt;- TITLE_BASICS |&gt; \n  filter (tconst %in% c(\"tt14833612\", \"tt15494038\", \"tt6342474\"))\n\nWhat TV series, with more than 12 episodes, has the highest average rating?\nANSWER: titletype = ‘tvseries’ does not contain episodenumber, unable to count episodes thus identify episodes with count grater then 12. However the ‘tvseries’ with highest rating is = Cumartesi-Pazar Surpriz\n\nhigh_avrg &lt;- full_join(TITLE_BASICS,TITLE_RATINGS, by = 'tconst')\n\nhigh_avrg_episode &lt;- full_join(TITLE_EPISODES,high_avrg, by = 'tconst')\n\nm = high_avrg_episode %&gt;% \n  select(tconst, episodeNumber, titleType, primaryTitle, averageRating) \n\nn &lt;- m |&gt; \n  filter(titleType==\"tvSeries\") |&gt; \n  group_by(primaryTitle, averageRating) |&gt; \n  count(primaryTitle,sort = TRUE) |&gt; \n  arrange(desc(averageRating))\n\nThe TV series Happy Days (1974-1984) gives us the common idiom “jump the shark”. The phrase comes from a controversial fifth season episode (aired in 1977) in which a lead character literally jumped over a shark on water skis. Idiomatically, it is used to refer to the moment when a once-great show becomes ridiculous and rapidly looses quality.\nIs it true that episodes from later seasons of Happy Days have lower average ratings than the early seasons?\nANSWER : Yes, it is true. Because the tvseries rating in 2008 have an average 5.4 rating, which is lower\n\nhappy_days &lt;- high_avrg_episode |&gt; \n  filter(primaryTitle == \"Happy Days\", titleType == \"tvSeries\")\n\n\n\n\nDesign a ‘success’ measure for IMDb entries, reflecting both quality and broad popular awareness. Implement your success metric using a mutate operator to add a new column to the TITLE_RATINGS table.\nChoose the top 5-10 movies on your metric and confirm that they were indeed box office successes.\nMy strategy is to use the highest ratings &gt; 9 and then filter to highest numVotes from there. It is how I get my top 5 below.\nANSWER: TOP 5 with high ratings, grater than 9.0, and with highest numVotes[IMDb entries], indicates success. See below success metrics\n1.The Shawshank Redemption,1994,Drama,ratings = 9.3, numVotes = 2953131.\n2. The Godfather,1972,Crime,Drama, ratings = 9.2, numVotes = 2058741\n3. The Chaos Class, 1975, Comedy,ratings = 9.2,numVotes = 43592\n4. Ramayana: The Legend of Prince Rama, 1993, Action,Adventure,Animation, ratings = 9.2, numVotes = 15433\n5. The Silence of Swastika, 2021, Documentary,History, ratings = 9.2, numVotes = 10567\nChoose 3-5 movies with large numbers of IMDb votes that score poorly on your success metric and confirm that they are indeed of low quality.# #My strategy is to get the worst ratings &lt; than 1-2, and then filter to highest numVotes to get my worst performers below.\nANSWER: TOP 3 with high numVotes [even grater than some of the top 5], and lowest rating, at less than 2.0 rating, indicates low quality.\n1.Radhe, 2021, Action,Crime,Thriller, ratings = 1.9, numVotes = 180240\n2.Sadak 2, 2020, Action,Drama, ratings = 1.2, numVotes = 96842\n3.Disaster Movie, 2008, Comedy,Sci-Fi, ratings = 1.9, numVotes = 95313\nPerform at least one other form of ‘spot check’ validation.\nI will use scatter plots using for my top 5 and low 3. Compare that to scatter plots with &lt; / &gt; 5 ratings- I am cutting at midpoint ratings because &lt; 2 or &gt; 9 may be too ambiguous - &lt;/&gt; 5 will even out. to keep data plot small, I am using 50 points.\n\n#Plot1 movies at &gt; 9 ratings, a trend is not clearly visible, looks like a straight horizontal line with a couple of outliers.\n\nsucess1 &lt;- full_join(TITLE_BASICS,TITLE_RATINGS, by = \"tconst\") |&gt; \n  select(tconst, titleType, primaryTitle, startYear,genres, averageRating,numVotes) |&gt; \n  filter(titleType == \"movie\", averageRating &gt; 9) |&gt; \n  group_by(primaryTitle) |&gt; \n  arrange(desc(numVotes)) \n\nplot1 = head(sucess1, 50) \n\nggplot(plot1, aes(x=averageRating, y=numVotes)) +\n  geom_point()\n\n\n\n\n\n\n\n#Plot2 movies at ratings &gt; 5, The grater the rating the higher the numVotes trend\n\nsucess2&lt;- full_join(TITLE_BASICS,TITLE_RATINGS, by = \"tconst\") |&gt; \n  select(tconst, titleType, primaryTitle, startYear,genres, averageRating,numVotes) |&gt; \n  filter(titleType == \"movie\", averageRating &gt; 5) |&gt; \n  group_by(primaryTitle) |&gt; \n  arrange(desc(numVotes)) \n\nplot2 = head(sucess2, 50)\n\nggplot(plot2, aes(x=averageRating, y=numVotes)) +\n  geom_point()\n\n\n\n\n\n\n\n#Below is the spot check for worst perfoming using same strategy for best perfomring [graphs]\n\n#PLOT3 movies at ratings &lt;2 - slightly see pattern of the higher the rating the higher numVotes, however there is a less numVote concentration\n\nsucess_not &lt;- full_join(TITLE_BASICS,TITLE_RATINGS, by = \"tconst\") |&gt; \n  select(tconst, titleType, primaryTitle, startYear,genres, averageRating,numVotes) |&gt; \n  filter(titleType == \"movie\", averageRating &lt; 2) |&gt; \n  group_by(primaryTitle) |&gt; \n  arrange(desc(numVotes)) \n\nplot3 = head(sucess_not, 50) \n\nggplot(plot3, aes(x=averageRating, y=numVotes)) +\n  geom_point()\n\n\n\n\n\n\n\n#PLOT 4 movies at ratings &lt; 5 - the higher the rating the grater the numVotes\n\nsucess_not2 &lt;- full_join(TITLE_BASICS,TITLE_RATINGS, by = \"tconst\") |&gt; \n  select(tconst, titleType, primaryTitle, startYear,genres, averageRating,numVotes) |&gt; \n  filter(titleType == \"movie\", averageRating &lt; 5) |&gt; \n  group_by(primaryTitle) |&gt; \n  arrange(desc(numVotes)) \n\nplot4 = head(sucess_not2, 50) \n\nggplot(plot4, aes(x=averageRating, y=numVotes)) + \n  geom_point() \n\n\n\n\n\n\n\n\nCome up with a numerical threshold for a project to be a ‘success’; that is, determine a value such that movies above are all “solid” or better.\nMy initial hypothesis was the higher the ratings the higher the numVotes will be. #I used an initial strategy where I picked very high ratings vs. very low ratings + spot check [graph] strategy where I cut data at midpoint ratings &lt;/&gt;5 ratings.\nAfter graphing both with a sample of 50 highest NumVotes, the graph strategy pointed to a better data visual, a clear trend. The results where visible following a trend where the higher the rating the higher the numVotes. #Unfortunatley strategy one was picking up multiple outliers, and could not base a recomendation on outliers only; a trend was not clearly visible with strategy 1.\nCombining both together. I have identified as movies &gt; 5 ratings with 2M NumVotes to be top performing movies. And movies at &lt; 5 ratings with &lt;25K numVotes as worst performers [for a shorter list of worst performers- low ratings with high numVotes will give me worst performers].\nFINAL SUCESS CODE AND PLOT\n\nFinal_sucess &lt;- full_join(TITLE_BASICS,TITLE_RATINGS, by = \"tconst\") |&gt; \n  select(tconst, titleType, primaryTitle, startYear,genres, averageRating,numVotes) |&gt; \n  filter(titleType == \"movie\", averageRating &gt; 5, numVotes &gt; 2000000) |&gt; \n  group_by(primaryTitle) |&gt; \n  arrange(desc(numVotes)) \n\nplot_final = head(Final_sucess, 50) \n\nggplot(plot_final, aes(x=averageRating, y=numVotes))  +\n  geom_point()\n\n\n\n\n\n\n\n\nFINAL SUCESS NOT CODE AND PLOT\n\nFinal_sucess_NOT &lt;- full_join(TITLE_BASICS,TITLE_RATINGS, by = \"tconst\") |&gt; \n  select(tconst, titleType, primaryTitle, startYear,genres, averageRating,numVotes) |&gt; \n  filter(titleType == \"movie\", averageRating &lt; 5, numVotes &lt; 25000) |&gt; \n  group_by(primaryTitle) |&gt; \n  arrange(desc(numVotes)) \n\nplot_final_not = head(Final_sucess_NOT, 50) \n\nggplot(plot_final_not, aes(x=averageRating, y=numVotes)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\nWhat was the genre with the most “successes” in each decade?\n\nsucess_decade &lt;- full_join(TITLE_BASICS,TITLE_RATINGS, by = \"tconst\") |&gt; \n  select(tconst, titleType, primaryTitle, startYear,genres, averageRating,numVotes) |&gt; \n  filter(titleType == \"movie\", averageRating &gt; 5, numVotes &gt; 1000000) |&gt; \n  group_by(primaryTitle) |&gt; \n  arrange(desc(numVotes)) |&gt; \n  separate_longer_delim(genres, \",\") \n\nplt = head(sucess_decade, 500)\n\nggplot(plt, aes(startYear, averageRating)) + \n  geom_point(aes(colour = factor(genres)))\n\n\n\n\n\n\n\n\nWhat genre consistently has the most “successes”? What genre used to reliably produced “successes” and has fallen out of favor?\nFrom 1970-2010 most successful genres appear to be drama & crime.\nWhat genre has produced the most “successes” since 2010? Does it have the highest success rate or does it only have a large number of successes because there are many productions in that genre?\nFrom 2010-2020 Sci-Fi had a tick up - success, at ratings higher than 8.5.\nWhat genre has become more popular in recent years?\nSci-Fi and Thrillers have risen in favor in most recent year\n\n\n\nIdentify (at least) two actors and one director who you will target as the key talent for your movie. Write a short “pitch” as to why they are likely to be successful. You should support your pitch with at least one graphic and one table.\nANSWER : Actor = Stephen King, for his popular movie ‘The Shawshank Redemption’ + Actress = Gundula Janowitz, for her popular movie ‘The Shawshank Redemption’.\n\nNAME_BASICS &lt;- NAME_BASICS |&gt;\n  rename_at('knownForTitles', ~'tconst')\n\nactors &lt;- NAME_BASICS |&gt; \n  separate_longer_delim(primaryProfession, \",\") |&gt; \n  separate_longer_delim(tconst, \",\") |&gt; \n  filter (primaryProfession == 'actor') |&gt; \n  select (tconst, primaryProfession, primaryName, birthYear, deathYear)\n\nactress &lt;- NAME_BASICS |&gt; \n  separate_longer_delim(primaryProfession, \",\") |&gt; \n  separate_longer_delim(tconst, \",\") |&gt; \n  filter (primaryProfession == 'actress') |&gt; \n  select (tconst, primaryProfession, primaryName, birthYear, deathYear)\n\nmovies &lt;- TITLE_BASICS |&gt; \n  separate_longer_delim(genres, \",\") |&gt; \n  separate_longer_delim(tconst, \",\") |&gt; \n  filter(titleType == 'movie') |&gt; \n  select(tconst, titleType, primaryTitle,genres)\n\nmovie_actor &lt;- full_join(actors,movies, by = 'tconst')\n\nWarning in full_join(actors, movies, by = \"tconst\"): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 1 of `x` matches multiple rows in `y`.\nℹ Row 11806 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\nmovie_actress &lt;- full_join(actress,movies, by = 'tconst')\n\nWarning in full_join(actress, movies, by = \"tconst\"): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 1 of `x` matches multiple rows in `y`.\nℹ Row 29522 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\npopular &lt;- TITLE_RATINGS |&gt; \n  separate_longer_delim(tconst, \",\")\n\npopular_actor_sucess &lt;- full_join(popular,movie_actor, by = 'tconst') |&gt; \n  filter(averageRating &gt; 5, numVotes &gt; 1000000) |&gt; \n  arrange(desc(numVotes)) |&gt; \n  slice_head(n=50) \n\npopular_actress_sucess &lt;- full_join(popular,movie_actress, by = 'tconst') |&gt; \n  filter(averageRating &gt; 5, numVotes &gt; 1000000) |&gt; \n  arrange(desc(numVotes)) |&gt; \n  slice_head(n=50) \n\n\n\n\nFind a classic movie to remake with your key talent. The original should have a large number of IMDb ratings, a high average rating, and not have been remade in the past 25 years.4\nOnce you have found your classic movie to remake, confirm whether key actors, directors, or writers from the original are still alive. If so, you need to contact your legal department to ensure they can secure the rights to the project. You may also want to include the classic actors as “fan service.”\nANSWER: I’m not a fan of remakes, so I would propose an entirely new movie based on the hit ‘The Shawshank Redemption’ - at the time it high very high ratings and votes. Give the people what they like. As a fan of Morgan Freeman and Stephen King, I would propose King write a new drama/thriller/inspirational, and Freeman star and produce it. Perfect combo and still alive doing great content. My team will contact them.\n\n\n\nIn today’s environment, the pattern has been to remake and create movies that resemble a ‘passing the torch’ feeling.\nTake for example a classic Hocus Pocus, or GhostBusters, or recet Bettle Juice, a recreation of the old. It start to look like a series and not a movie.\nWhy not create an entirely new generation of movies, to both inspire the new generations while at the same time rekindle the nostalgia of the older generations.\nI propose a making of a new type of DRAMA, DRAMA101 - Where our movies will contain multiple genres, from drama, thriller [for oldies], inspirational [for sensitivity of today’s environment], touch of comedy [reflecting real life]. A mix. New.\nWhy not reflect the different stages/ feelings in one movie?\n“Inside Out” you may be thinking…“childish” you say…\nNO…rather this is reality, a movie should reflect a reality of life. A new Genre.\nI propose to create a movie similar to the greatest hit ‘The Shawshank Redemption’, Inspired by Stephen King’s book “Rita Hayworth and Shawshank Redemption”, Produced and stared by Morgan Freeman , one of the greatest inspirational person in media now.\nThe movie could follow a similar plot as Stephen King book to start, innocent man/woman in jail, finds purpose in life. But instead of a killing it could be another type of crime, like crossing a border, or robbing to survive, or falsely accusations.\nThe name for the movie could be “Innocent until proven Guilty” narrated by Morgan Freeman and Edith Mathis. Stared by a fresh set of new diverse actors, never seen actors - the normal everyday person as an actor. Are you ready to bring the next genre of the movie future.\n‘Out with the old and in with the new.’"
  },
  {
    "objectID": "mp02.html#task1---column-type-correction",
    "href": "mp02.html#task1---column-type-correction",
    "title": "Mini-Project #02:The Business of Show Business",
    "section": "TASK1 - Column type correction",
    "text": "TASK1 - Column type correction\n```{r}\nTITLE_BASICS &lt;- TITLE_BASICS |&gt; mutate(startYear = as.numeric(startYear), runtimeMinutes = as.numeric(runtimeMinutes), endYear = as.numeric(endYear))\nTITLE_CREW &lt;- TITLE_CREW |&gt; mutate(writers = as.logical(writers))\nTITLE_EPISODES &lt;- TITLE_EPISODES |&gt; mutate(seasonNumber = as.numeric(seasonNumber), episodeNumber = as.numeric(episodeNumber))\nTITLE_PRINCIPALS[TITLE_PRINCIPALS == “\\N”] &lt;- NA\n```\nCODE BELOW SEPARATES MULTIPLE VALUES IN A STRING USING A DELIMINATE [I.E ‘,’], TO BE USED IN LATER ANALYSIS\n```{r}\nNAME_BASICS |&gt; separate_longer_delim(knownForTitles, “,”) |&gt; slice_head(n=10)\n```\nTASK 2 - QUESTIONS\nHow many movies are in our data set? How many TV series? How many TV episodes?\n```{r}\n#unique(df$titleType), find out how many unique titletypes exist in title_basics #TITLE_BASICS has columns primarytitle and primary title to use for movie title, use group by to separate ‘movies’, ‘tvseries’, tvepisodes’ # ANSWER : MOVIES TOTAL = 118546, TV series = 28745 , tv episodes = 121731\ndf = TITLE_BASICS %&gt;% group_by(titleType, primaryTitle) %&gt;% summarize(movie_total = sum(str_count(titleType, “movie”)), tvepisode_total = sum(str_count(titleType, “tvEpisode”)), tvSeries_total = sum(str_count(titleType, “tvSeries”))) a = filter(df,titleType == “movie”) b = filter(df,titleType == “tvEpisode”) c = filter(df,titleType == “tvSeries”)\n```\nWho is the oldest living person in our data set?\nNeed name, birth year = oldest and deathyear= NA\nANSWER : Assuming the oldest person alive is represented by deathYear = NA, for the oldest person alive greater than 1917 [oldest person alive] ,from our data set, there are 89 people still alive\n```{r}\noldest_person &lt;- NAME_BASICS |&gt; filter(birthYear &gt; 1917, is.na(deathYear)) |&gt; arrange(birthYear) |&gt; slice_head(n=89) #There are 89 names born on 1918, deathYear= NA\n```\nThere is one TV Episode in this data set with a perfect 10/10 rating and at least 200,000 IMDb ratings. What is it? What series does it belong to?\n#Join tilebasics where titletype = ‘tvepisode’ to titleratings using tconst, filter average rating = 10 #ANSWER : The TV episode with perfect 10 rating, at 230087 votes, is Ozymandias at seson 5, episode 14\n```{r}\nrating &lt;- TITLE_RATINGS |&gt; filter(averageRating == 10, numVotes &gt;= 200000) #This gives me tconst where rating =10 + &gt;200000\ntitle_a &lt;- TITLE_EPISODES |&gt; filter(tconst == “tt2301451”) # identify for unique episode\ntitle_b &lt;- TITLE_BASICS |&gt; filter(tconst == “tt2301451”) # identify for unique episode\nd = rating %&gt;% left_join(title_a, by = “tconst”) #join for full view\ne = d %&gt;% left_join(title_b, by = “tconst”) #further join for full view\n```\nWhat four projects is the actor Mark Hamill most known for?\nuse name basics to filter to actor Mark Hamil , use titlebasics to get tittle of projects\nANSWER: Actor Hamil is known for Castlevania: Nocturne [tvSeries], Drifting Home [movie], Boruto: Naruto Next Generations [ tvSeries]\n```{r}\nactor_Hamil &lt;- NAME_BASICS |&gt; filter(primaryName == “Hamil”) |&gt; separate_longer_delim(knownForTitles, “,”)\ntitle_Hamil &lt;- TITLE_BASICS |&gt; filter (tconst %in% c(“tt14833612”, “tt15494038”, “tt6342474”))\n```\nWhat TV series, with more than 12 episodes, has the highest average rating?\n#ANSWER: titletype = ‘tvseries’ does not contain episodenumber, unable to count episodes thus identify episodes with count grater then 12. However the ‘tvseries’ with highest rating is = Cumartesi-Pazar Surpriz\n```{r}\nhigh_avrg &lt;- full_join(TITLE_BASICS,TITLE_RATINGS, by = ‘tconst’)\nhigh_avrg_episode &lt;- full_join(TITLE_EPISODES,high_avrg, by = ‘tconst’)\nm = high_avrg_episode %&gt;% select(tconst, episodeNumber, titleType, primaryTitle, averageRating) n &lt;- m |&gt; filter(titleType==“tvSeries”) |&gt; group_by(primaryTitle, averageRating) |&gt; count(primaryTitle,sort = TRUE) |&gt; arrange(desc(averageRating))\n```\nThe TV series Happy Days (1974-1984) gives us the common idiom “jump the shark”. The phrase comes from a controversial fifth season episode (aired in 1977) in which a lead character literally jumped over a shark on water skis. Idiomatically, it is used to refer to the moment when a once-great show becomes ridiculous and rapidly looses quality.\nIs it true that episodes from later seasons of Happy Days have lower average ratings than the early seasons?\nANSWER : Yes, it is true. Because the tvseries atrting in 2008 have an average 5.4 rating, whihc is lower\n```{r}\nhappy_days &lt;- high_avrg_episode |&gt; filter(primaryTitle == “Happy Days”, titleType == “tvSeries”)\n```"
  },
  {
    "objectID": "mp02.html#task-3---customer-success-metric",
    "href": "mp02.html#task-3---customer-success-metric",
    "title": "Mini-Project #02:The Business of Show Business",
    "section": "TASK 3 - CUSTOMER SUCCESS METRIC",
    "text": "TASK 3 - CUSTOMER SUCCESS METRIC\nDesign a ‘success’ measure for IMDb entries, reflecting both quality and broad popular awareness. Implement your success metric using a mutate operator to add a new column to the TITLE_RATINGS table.\n\nChoose the top 5-10 movies on your metric and confirm that they were indeed box office successes. #My strategy is to use the highest ratings &gt; 9 and then filter to highest numVotes from there. It is how I get my top 5 below.\nANSWER: TOP 5 with high ratings, grater than 9.0, and with highest numVotes[IMDb entries], indicates success. See below success metrics\n#1.The Shawshank Redemption,1994,Drama,ratings = 9.3, numVotes = 2953131.\n#2. The Godfather,1972,Crime,Drama, ratings = 9.2, numVotes = 2058741\n#3. The Chaos Class, 1975, Comedy,ratings = 9.2,numVotes = 43592\n#4. Ramayana: The Legend of Prince Rama, 1993, Action,Adventure,Animation, ratings = 9.2, numVotes = 15433\n#5. The Silence of Swastika, 2021, Documentary,History, ratings = 9.2, numVotes = 10567\n\n\nChoose 3-5 movies with large numbers of IMDb votes that score poorly on your success metric and confirm that they are indeed of low quality.# #My strategy is to get the worst ratings &lt; than 1-2, and then filter to highest numVotes to get my worst performers below.\n#ANSWER: TOP 3 with high numVotes [even grater than some of the top 5], and lowest rating, at less than 2.0 rating, indicates low quality.\n#1.Radhe, 2021, Action,Crime,Thriller, ratings = 1.9, numVotes = 180240\n#2.Sadak 2, 2020, Action,Drama, ratings = 1.2, numVotes = 96842\n#3.Disaster Movie, 2008, Comedy,Sci-Fi, ratings = 1.9, numVotes = 95313\n\n\nPerform at least one other form of ‘spot check’ validation.\nI will use scatter plots using for my top 5 and low 3. Compare that to scatter plots with &lt; / &gt; 5 ratings- I am cutting at midpoint ratings because &lt; 2 or &gt; 9 may be too ambiguous - &lt;/&gt; 5 will even out. to keep data plot small, I am using 50 points.\n```{r}\n#Plot1 movies at &gt; 9 ratings, a trend is not clearly visible, looks like a straight horizontal line with a couple of outliers.\nsucess1 &lt;- full_join(TITLE_BASICS,TITLE_RATINGS, by = “tconst”) |&gt; select(tconst, titleType, primaryTitle, startYear,genres, averageRating,numVotes) |&gt; filter(titleType == “movie”, averageRating &gt; 9) |&gt; group_by(primaryTitle) |&gt; arrange(desc(numVotes)) view(sucess)\nplot1 = head(sucess1, 50) view(plot1)\nggplot(plot1, aes(x=averageRating, y=numVotes)) + #Plot &gt;9 ratings, comparison plot geom_point(size = 1)\n#Plot2 movies at ratings &gt; 5, The grater the rating the higher the numVotes trend\nsucess2&lt;- full_join(TITLE_BASICS,TITLE_RATINGS, by = “tconst”) |&gt; select(tconst, titleType, primaryTitle, startYear,genres, averageRating,numVotes) |&gt; filter(titleType == “movie”, averageRating &gt; 5) |&gt; group_by(primaryTitle) |&gt; arrange(desc(numVotes)) view(sucess2)\nplot2 = head(sucess2, 50) view(plot2)\nggplot(plot2, aes(x=averageRating, y=numVotes)) + #Plot &gt;5 ratings, comparison plot geom_point()\n#Below is the spot check for worst perfoming using same strategy for best perfomring [graphs]\nPLOT3 movies at ratings &lt;2 - slightly see pattern of the higher the rating the higher numVotes, however there is a less numVote concentration\nsucess_not &lt;- full_join(TITLE_BASICS,TITLE_RATINGS, by = “tconst”) |&gt; select(tconst, titleType, primaryTitle, startYear,genres, averageRating,numVotes) |&gt; filter(titleType == “movie”, averageRating &lt; 2) |&gt; group_by(primaryTitle) |&gt; arrange(desc(numVotes)) view(sucess_not)\nplot3 = head(sucess_not, 50) view(plot3)\nggplot(plot3, aes(x=averageRating, y=numVotes)) + #Plot &lt;2 ratings, comparison plot geom_point()\n#PLOT 4 movies at ratings &lt; 5 - the higher the rating the grater the numVotes\nsucess_not2 &lt;- full_join(TITLE_BASICS,TITLE_RATINGS, by = “tconst”) |&gt; select(tconst, titleType, primaryTitle, startYear,genres, averageRating,numVotes) |&gt; filter(titleType == “movie”, averageRating &lt; 5) |&gt; group_by(primaryTitle) |&gt; arrange(desc(numVotes)) view(sucess_not2)\nplot4 = head(sucess_not2, 50) view(plot4)\n``` { r\nggplot(plot4, aes(x=averageRating, y=numVotes)) + #Plot &lt;2 ratings, comparison plot geom_point() }\n```\n\n\nCome up with a numerical threshold for a project to be a ‘success’; that is, determine a value such that movies above are all “solid” or better.\nMy initial hypothesis was the higher the ratings the higher the numVotes will be. #I used an initial strategy where I picked very high ratings vs. very low ratings + spot check [graph] strategy where I cut data at midpoint ratings &lt;/&gt;5 ratings.\nAfter graphing both with a sample of 50 highest NumVotes, the graph strategy pointed to a better data visual, a clear trend. The results where visible following a trend where the higher the rating the higher the numVotes. #Unfortunatley strategy one was picking up multiple outliers, and could not base a recomendation on outliers only; a trend was not clearly visible with strategy 1.\nCombining both together. I have identified as movies &gt; 5 ratings with 2M NumVotes to be top performing movies. And movies at &lt; 5 ratings with &lt;25K numVotes as worst performers [for a shorter list of worst performers- low ratings with high numVotes will give me worst performers].\n\n\nFINAL SUCESS CODE AND PLOT\n```{r}\nFinal_sucess &lt;- full_join(TITLE_BASICS,TITLE_RATINGS, by = “tconst”) |&gt; select(tconst, titleType, primaryTitle, startYear,genres, averageRating,numVotes) |&gt; filter(titleType == “movie”, averageRating &gt; 5, numVotes &gt; 2000000) |&gt; group_by(primaryTitle) |&gt; arrange(desc(numVotes)) view(Final_sucess)\nplot_final = head(Final_sucess, 50) view(plot_final)\nggplot(plot_final, aes(x=averageRating, y=numVotes)) + #Plot &gt; 5, &gt; 2M+ numvotes geom_point()\n```\n\n\nFINAL SUCESS NOT CODE AND PLOT\n```{r}\nFinal_sucess_NOT &lt;- full_join(TITLE_BASICS,TITLE_RATINGS, by = “tconst”) |&gt; select(tconst, titleType, primaryTitle, startYear,genres, averageRating,numVotes) |&gt; filter(titleType == “movie”, averageRating &lt; 5, numVotes &lt; 25000) |&gt; group_by(primaryTitle) |&gt; arrange(desc(numVotes)) view(Final_sucess_NOT)\nplot_final_not = head(Final_sucess_NOT, 50) view(plot_final_not)\nggplot(plot_final_not, aes(x=averageRating, y=numVotes)) + #Plot &lt; 5, &lt; 25k numvotes geom_point()\n```"
  },
  {
    "objectID": "mp02.html#task-4---examining-success-by-genre-and-decade",
    "href": "mp02.html#task-4---examining-success-by-genre-and-decade",
    "title": "Mini-Project #02:The Business of Show Business",
    "section": "TASK 4 - Examining Success by Genre and Decade",
    "text": "TASK 4 - Examining Success by Genre and Decade\nWhat was the genre with the most “successes” in each decade?\n```{r}\nsucess_decade &lt;- full_join(TITLE_BASICS,TITLE_RATINGS, by = “tconst”) |&gt; select(tconst, titleType, primaryTitle, startYear,genres, averageRating,numVotes) |&gt; filter(titleType == “movie”, averageRating &gt; 5, numVotes &gt; 1000000) |&gt; group_by(primaryTitle) |&gt; arrange(desc(numVotes)) |&gt; separate_longer_delim(genres, “,”) view(sucess_decade)\nplt = head(sucess_decade, 500) view(plt)\nggplot(plt, aes(startYear, averageRating)) + geom_point(aes(colour = factor(genres)))\n```\n#What genre consistently has the most “successes”? What genre used to reliably produced “successes” and has fallen out of favor?\nFrom 1970-2010 most successful genres appear to be drama & crime.\n#What genre has produced the most “successes” since 2010? Does it have the highest success rate or does it only have a large number of successes because there are many productions in that genre?\nFrom 2010-2020 Sci-Fi had a tick up - sucess, at ratings higher than 8.5.\n#What genre has become more popular in recent years?\nSci-Fi and Thrillers have risen in favor in most recent year\n\nTASK 5 - Successful Personnel in the Genre\nIdentify (at least) two actors and one director who you will target as the key talent for your movie. Write a short “pitch” as to why they are likely to be successful. You should support your pitch with at least one graphic and one table.\n#ANSWER : Actor = Stephen King, for his popular movie ‘The Shawshank Redemption’ + Actress = Gundula Janowitz, for her popular movie ‘The Shawshank Redemption’.\n```{r}\nactors &lt;- NAME_BASICS |&gt; separate_longer_delim(primaryProfession, “,”) |&gt; separate_longer_delim(tconst, “,”) |&gt; filter (primaryProfession == ‘actor’) |&gt; select (tconst, primaryProfession, primaryName, birthYear, deathYear)\nactress &lt;- NAME_BASICS |&gt; separate_longer_delim(primaryProfession, “,”) |&gt; separate_longer_delim(tconst, “,”) |&gt; filter (primaryProfession == ‘actress’) |&gt; select (tconst, primaryProfession, primaryName, birthYear, deathYear)\nmovies &lt;- TITLE_BASICS |&gt; separate_longer_delim(genres, “,”) |&gt; separate_longer_delim(tconst, “,”) |&gt; filter(titleType == ‘movie’) |&gt; select(tconst, titleType, primaryTitle,genres)\nmovie_actor &lt;- full_join(actors,movies, by = ‘tconst’)\nmovie_actress &lt;- full_join(actress,movies, by = ‘tconst’)\npopular &lt;- TITLE_RATINGS |&gt; separate_longer_delim(tconst, “,”)\npopular_actor_sucess &lt;- full_join(popular,movie_actor, by = ‘tconst’) |&gt; filter(averageRating &gt; 5, numVotes &gt; 1000000) |&gt; arrange(desc(numVotes)) |&gt; slice_head(n=50) View(popular_actor_sucess )\npopular_actress_sucess &lt;- full_join(popular,movie_actress, by = ‘tconst’) |&gt; filter(averageRating &gt; 5, numVotes &gt; 1000000) |&gt; arrange(desc(numVotes)) |&gt; slice_head(n=50) View(popular_actress_sucess )\n```\n\n\nTASK 6 - Nostalgia and Remakes\nFind a classic movie to remake with your key talent. The original should have a large number of IMDb ratings, a high average rating, and not have been remade in the past 25 years.4 #Once you have found your classic movie to remake, confirm whether key actors, directors, or writers from the original are still alive. If so, you need to contact your legal department to ensure they can secure the rights to the project. You may also want to include the classic actors as “fan service.”\n#ANSWER: I’m not a fan of remakes, so I would propose an entirely new movie based on the hit ‘The Shawshank Redemption’ - at the time it high very high ratings and votes. Give the people what they like. # As a fan of Morgan Freeman and Stephen King, I would propose King write a new drama/thiller/inpirational, and Freeman star and produce it. Perfect combo and still alive doing great content. My team will contact them.\n\nTASK 7 - PITCH and DELIVERY\nToday’s environment the pattern has been remaking and creating movies that ‘passes the torch’ to the youth.\nTake for example a classic Hocus Pocus, or GhostBusters, or recet Bettle Juice. Where older actors towards the end are joined by newer actors whom appear to be the new starts of that movie. Boring!\nWhy follow when we can create an entirely new generation of movies, to inspire the new generations while at the same time bring rekindle the nostalgia of the older generations.\nI propose a making of a new type of DRAMA, DRAMA101 - Where our movie will contain multiple genres, drama, thriller [for oldies], inspirational [for sensitivity of today’s environment], touch of comedy [reflecting real life].\nNot all is sad, mad, happy, joy. Why not reflect the different stages/ feelings in one movie. “Inside Out” you may be thinking…“childish” you say…\nNO…rather this is reality, a movie should reflect a reality of life. A new Genre.\nI propose to create a movie similar to the greatest hit ‘The Shawshank Redemption’, Inspired by Stephen King’s book “Rita Hayworth and Shawshank Redemption”, Produced and stared by Morgan Freeman , one of the greatest inspirational person in media now.\nThe movie could follow a similar plot as Stephen King book, innocent man/woman in jail, finds purpose in life. But instead of a killing it could be another type of crime, like crossing a border, or robbing to survive, or falsely accusations.\nThe name for the movie will be “Innocent until proven Guilty” narrated by Morgan Freeman and Edith Mathis. Stared by a fresh set of new diverse actors, never seen actors - the normal everyday person as an actor. Are you ready to bring the next genre of the movie future. Out with the old and in with the new."
  },
  {
    "objectID": "mp02.html#welcome-to-project-2-by-alex",
    "href": "mp02.html#welcome-to-project-2-by-alex",
    "title": "Mini-Project #02:The Business of Show Business",
    "section": "",
    "text": "Flower\n\n\n\nlibrary(tidyverse) \nlibrary(ggplot2)\n\n#| label: 'imdb_name_basics' \n#| message: false \n#| warning: false \n#| cache: true\n\nget_imdb_file &lt;- function(fname){ \n  BASE_URL &lt;- \"https://datasets.imdbws.com/\" \n  fname_ext &lt;- paste0(fname, \".tsv.gz\") \n  if(!file.exists(fname_ext)){ \n    FILE_URL &lt;- paste0(BASE_URL, fname_ext) \n    download.file(FILE_URL, \n                  destfile = fname_ext) \n} \nas.data.frame(readr::read_tsv(fname_ext, lazy=FALSE)) \n}\n\nNAME_BASICS &lt;- get_imdb_file(\"name.basics\")\n\nRows: 13886887 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (6): nconst, primaryName, birthYear, deathYear, primaryProfession, known...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#| label: 'imdb_title_basics' \n#| message: false \n#| warning: false \n#| cache: true \nTITLE_BASICS &lt;- get_imdb_file(\"title.basics\")\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 11176312 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (8): tconst, titleType, primaryTitle, originalTitle, startYear, endYear,...\ndbl (1): isAdult\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#| label: 'imdb_title_episode' \n#| message: false \n#| warning: false \n#| cache: true \n\nTITLE_EPISODES &lt;- get_imdb_file(\"title.episode\")\n\nRows: 8580365 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (4): tconst, parentTconst, seasonNumber, episodeNumber\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#| label: 'imdb_title_ratings' \n#| message: false \n#| warning: false \n#| cache: true \n\nTITLE_RATINGS &lt;- get_imdb_file(\"title.ratings\")\n\nRows: 1489867 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (1): tconst\ndbl (2): averageRating, numVotes\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#| label: 'imdb_title_crew' \n#| message: false \n#| warning: false \n#| cache: true \n\nTITLE_CREW &lt;- get_imdb_file(\"title.crew\")\n\nRows: 10515996 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (3): tconst, directors, writers\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#| label: 'imdb_title_principals' \n#| eval: false \n#| message: false \n#| warning: false \n#| cache: false \n\nTITLE_PRINCIPALS &lt;- get_imdb_file(\"title.principals\")\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 86651930 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (5): tconst, nconst, category, job, characters\ndbl (1): ordering\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#| label: 'name_basics_filter' \n#| cache: true \n \nNAME_BASICS &lt;- NAME_BASICS |&gt; \n  filter(str_count(knownForTitles, \",\") &gt; 1)\n\n#| label: 'title_ratings_tail' \n\nTITLE_RATINGS |&gt; ggplot(aes(x=numVotes)) + \n  geom_histogram(bins=30) + \n  xlab(\"Number of IMDB Ratings\") + \n  ylab(\"Number of Titles\") + \n  ggtitle(\"Majority of IMDB Titles Have Less than 100 Ratings\") + \n  theme_bw() + \n  scale_x_log10(label=scales::comma) + \n  scale_y_continuous(label=scales::comma)\n\n\n\n\n\n\n\n#| label: 'title_ratings_quantile' \n\nTITLE_RATINGS |&gt; pull(numVotes) |&gt; \n  quantile()\n\n     0%     25%     50%     75%    100% \n      5      11      26     100 2953131 \n\n#| label: 'title_ratings_filter' \n#| cache: true \n \nTITLE_RATINGS &lt;- TITLE_RATINGS |&gt; \n  filter(numVotes &gt;= 100)\n\n#| cache: true \n#| label: 'title_other_filter' \n#| message: false \n \nTITLE_BASICS &lt;- TITLE_BASICS |&gt; semi_join(TITLE_RATINGS, join_by(tconst == tconst))\n\nTITLE_CREW &lt;- TITLE_CREW |&gt; semi_join(TITLE_RATINGS, join_by(tconst == tconst))\n\nTITLE_EPISODES_1 &lt;- TITLE_EPISODES |&gt; semi_join(TITLE_RATINGS, join_by(tconst == tconst)) \n\nTITLE_EPISODES_2 &lt;- TITLE_EPISODES |&gt; semi_join(TITLE_RATINGS, join_by(parentTconst == tconst))\n\nTITLE_EPISODES &lt;- bind_rows(TITLE_EPISODES_1, TITLE_EPISODES_2) |&gt; distinct()\n\nTITLE_PRINCIPALS &lt;- TITLE_PRINCIPALS |&gt; semi_join(TITLE_RATINGS, join_by(tconst == tconst))\n\nrm(TITLE_EPISODES_1) \nrm(TITLE_EPISODES_2)\n\nNAME_BASICS &lt;- NAME_BASICS |&gt; \n  mutate(birthYear = as.numeric(birthYear), deathYear = as.numeric(deathYear))\n\nWarning: There were 2 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `birthYear = as.numeric(birthYear)`.\nCaused by warning:\n! NAs introduced by coercion\nℹ Run `dplyr::last_dplyr_warnings()` to see the 1 remaining warning.\n\n\n\n\n\nTITLE_BASICS &lt;- TITLE_BASICS |&gt; \n  mutate(startYear = as.numeric(startYear), runtimeMinutes = as.numeric(runtimeMinutes), endYear = as.numeric(endYear))\n\nWarning: There were 3 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `startYear = as.numeric(startYear)`.\nCaused by warning:\n! NAs introduced by coercion\nℹ Run `dplyr::last_dplyr_warnings()` to see the 2 remaining warnings.\n\nTITLE_CREW &lt;- TITLE_CREW |&gt; \n  mutate(writers = as.logical(writers))\n\nTITLE_EPISODES &lt;- TITLE_EPISODES |&gt; \n  mutate(seasonNumber = as.numeric(seasonNumber), episodeNumber = as.numeric(episodeNumber))\n\nWarning: There were 2 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `seasonNumber = as.numeric(seasonNumber)`.\nCaused by warning:\n! NAs introduced by coercion\nℹ Run `dplyr::last_dplyr_warnings()` to see the 1 remaining warning.\n\nTITLE_PRINCIPALS[TITLE_PRINCIPALS == \"\\\\N\"] &lt;- NA\n\n\n\n\nHow many movies are in our data set? How many TV series? How many TV episodes?\nANSWER : MOVIES TOTAL = 118546, TV series = 28745 , tv episodes = 121731\n\ndf = TITLE_BASICS %&gt;% group_by(titleType, primaryTitle) %&gt;% \n    summarize(movie_total = sum(str_count(titleType, \"movie\")), \n            tvepisode_total = sum(str_count(titleType, \"tvEpisode\")), \n            tvSeries_total = sum(str_count(titleType, \"tvSeries\"))) \n\n`summarise()` has grouped output by 'titleType'. You can override using the\n`.groups` argument.\n\na = filter(df,titleType == \"movie\") \nb = filter(df,titleType == \"tvEpisode\") \nc = filter(df,titleType == \"tvSeries\")\n\nWho is the oldest living person in our data set?\nNeed name, birth year = oldest and deathyear= NA\nANSWER : Assuming the oldest person alive is represented by deathYear = NA, for the oldest person alive greater than 1917 [oldest person alive] ,from our data set, there are 89 people still alive\n\noldest_person &lt;- NAME_BASICS |&gt; \n  filter(birthYear &gt; 1917, is.na(deathYear)) |&gt; \n  arrange(birthYear) |&gt; \n  slice_head(n=89) #There are 89 names born on 1918, deathYear= NA\n\nThere is one TV Episode in this data set with a perfect 10/10 rating and at least 200,000 IMDb ratings. What is it? What series does it belong to?\nI Joined tilebasics where titletype = ‘tvepisode’ to titleratings using tconst, filter average rating = 10 ANSWER : The TV episode with perfect 10 rating, at 230087 votes, is Ozymandias at seson 5, episode 14\n\nrating &lt;- TITLE_RATINGS |&gt; \n  filter(averageRating == 10, numVotes &gt;= 200000) #This gives me tconst where rating =10 + &gt;200000\n\ntitle_a &lt;- TITLE_EPISODES |&gt; \n  filter(tconst == \"tt2301451\") # identify for unique episode\n\ntitle_b &lt;- TITLE_BASICS |&gt; \n  filter(tconst == \"tt2301451\") # identify for unique episode\n\nd = rating %&gt;% left_join(title_a, by = \"tconst\") #join for full view\n\ne = d %&gt;% left_join(title_b, by = \"tconst\") #further join for full view\n\nWhat four projects is the actor Mark Hamill most known for?\nI used name basics to filter to actor Mark Hamil , used titlebasics to get tittle of projects\nANSWER: Actor Hamil is known for Castlevania: Nocturne [tvSeries], Drifting Home [movie], Boruto: Naruto Next Generations [ tvSeries]\n\nactor_Hamil &lt;- NAME_BASICS |&gt; \n  filter(primaryName == \"Hamil\") |&gt; \n  separate_longer_delim(knownForTitles, \",\")\n\ntitle_Hamil &lt;- TITLE_BASICS |&gt; \n  filter (tconst %in% c(\"tt14833612\", \"tt15494038\", \"tt6342474\"))\n\nWhat TV series, with more than 12 episodes, has the highest average rating?\nANSWER: titletype = ‘tvseries’ does not contain episodenumber, unable to count episodes thus identify episodes with count grater then 12. However the ‘tvseries’ with highest rating is = Cumartesi-Pazar Surpriz\n\nhigh_avrg &lt;- full_join(TITLE_BASICS,TITLE_RATINGS, by = 'tconst')\n\nhigh_avrg_episode &lt;- full_join(TITLE_EPISODES,high_avrg, by = 'tconst')\n\nm = high_avrg_episode %&gt;% \n  select(tconst, episodeNumber, titleType, primaryTitle, averageRating) \n\nn &lt;- m |&gt; \n  filter(titleType==\"tvSeries\") |&gt; \n  group_by(primaryTitle, averageRating) |&gt; \n  count(primaryTitle,sort = TRUE) |&gt; \n  arrange(desc(averageRating))\n\nThe TV series Happy Days (1974-1984) gives us the common idiom “jump the shark”. The phrase comes from a controversial fifth season episode (aired in 1977) in which a lead character literally jumped over a shark on water skis. Idiomatically, it is used to refer to the moment when a once-great show becomes ridiculous and rapidly looses quality.\nIs it true that episodes from later seasons of Happy Days have lower average ratings than the early seasons?\nANSWER : Yes, it is true. Because the tvseries rating in 2008 have an average 5.4 rating, which is lower\n\nhappy_days &lt;- high_avrg_episode |&gt; \n  filter(primaryTitle == \"Happy Days\", titleType == \"tvSeries\")\n\n\n\n\nDesign a ‘success’ measure for IMDb entries, reflecting both quality and broad popular awareness. Implement your success metric using a mutate operator to add a new column to the TITLE_RATINGS table.\nChoose the top 5-10 movies on your metric and confirm that they were indeed box office successes.\nMy strategy is to use the highest ratings &gt; 9 and then filter to highest numVotes from there. It is how I get my top 5 below.\nANSWER: TOP 5 with high ratings, grater than 9.0, and with highest numVotes[IMDb entries], indicates success. See below success metrics\n1.The Shawshank Redemption,1994,Drama,ratings = 9.3, numVotes = 2953131.\n2. The Godfather,1972,Crime,Drama, ratings = 9.2, numVotes = 2058741\n3. The Chaos Class, 1975, Comedy,ratings = 9.2,numVotes = 43592\n4. Ramayana: The Legend of Prince Rama, 1993, Action,Adventure,Animation, ratings = 9.2, numVotes = 15433\n5. The Silence of Swastika, 2021, Documentary,History, ratings = 9.2, numVotes = 10567\nChoose 3-5 movies with large numbers of IMDb votes that score poorly on your success metric and confirm that they are indeed of low quality.# #My strategy is to get the worst ratings &lt; than 1-2, and then filter to highest numVotes to get my worst performers below.\nANSWER: TOP 3 with high numVotes [even grater than some of the top 5], and lowest rating, at less than 2.0 rating, indicates low quality.\n1.Radhe, 2021, Action,Crime,Thriller, ratings = 1.9, numVotes = 180240\n2.Sadak 2, 2020, Action,Drama, ratings = 1.2, numVotes = 96842\n3.Disaster Movie, 2008, Comedy,Sci-Fi, ratings = 1.9, numVotes = 95313\nPerform at least one other form of ‘spot check’ validation.\nI will use scatter plots using for my top 5 and low 3. Compare that to scatter plots with &lt; / &gt; 5 ratings- I am cutting at midpoint ratings because &lt; 2 or &gt; 9 may be too ambiguous - &lt;/&gt; 5 will even out. to keep data plot small, I am using 50 points.\n\n#Plot1 movies at &gt; 9 ratings, a trend is not clearly visible, looks like a straight horizontal line with a couple of outliers.\n\nsucess1 &lt;- full_join(TITLE_BASICS,TITLE_RATINGS, by = \"tconst\") |&gt; \n  select(tconst, titleType, primaryTitle, startYear,genres, averageRating,numVotes) |&gt; \n  filter(titleType == \"movie\", averageRating &gt; 9) |&gt; \n  group_by(primaryTitle) |&gt; \n  arrange(desc(numVotes)) \n\nplot1 = head(sucess1, 50) \n\nggplot(plot1, aes(x=averageRating, y=numVotes)) +\n  geom_point()\n\n\n\n\n\n\n\n#Plot2 movies at ratings &gt; 5, The grater the rating the higher the numVotes trend\n\nsucess2&lt;- full_join(TITLE_BASICS,TITLE_RATINGS, by = \"tconst\") |&gt; \n  select(tconst, titleType, primaryTitle, startYear,genres, averageRating,numVotes) |&gt; \n  filter(titleType == \"movie\", averageRating &gt; 5) |&gt; \n  group_by(primaryTitle) |&gt; \n  arrange(desc(numVotes)) \n\nplot2 = head(sucess2, 50)\n\nggplot(plot2, aes(x=averageRating, y=numVotes)) +\n  geom_point()\n\n\n\n\n\n\n\n#Below is the spot check for worst perfoming using same strategy for best perfomring [graphs]\n\n#PLOT3 movies at ratings &lt;2 - slightly see pattern of the higher the rating the higher numVotes, however there is a less numVote concentration\n\nsucess_not &lt;- full_join(TITLE_BASICS,TITLE_RATINGS, by = \"tconst\") |&gt; \n  select(tconst, titleType, primaryTitle, startYear,genres, averageRating,numVotes) |&gt; \n  filter(titleType == \"movie\", averageRating &lt; 2) |&gt; \n  group_by(primaryTitle) |&gt; \n  arrange(desc(numVotes)) \n\nplot3 = head(sucess_not, 50) \n\nggplot(plot3, aes(x=averageRating, y=numVotes)) +\n  geom_point()\n\n\n\n\n\n\n\n#PLOT 4 movies at ratings &lt; 5 - the higher the rating the grater the numVotes\n\nsucess_not2 &lt;- full_join(TITLE_BASICS,TITLE_RATINGS, by = \"tconst\") |&gt; \n  select(tconst, titleType, primaryTitle, startYear,genres, averageRating,numVotes) |&gt; \n  filter(titleType == \"movie\", averageRating &lt; 5) |&gt; \n  group_by(primaryTitle) |&gt; \n  arrange(desc(numVotes)) \n\nplot4 = head(sucess_not2, 50) \n\nggplot(plot4, aes(x=averageRating, y=numVotes)) + \n  geom_point() \n\n\n\n\n\n\n\n\nCome up with a numerical threshold for a project to be a ‘success’; that is, determine a value such that movies above are all “solid” or better.\nMy initial hypothesis was the higher the ratings the higher the numVotes will be. #I used an initial strategy where I picked very high ratings vs. very low ratings + spot check [graph] strategy where I cut data at midpoint ratings &lt;/&gt;5 ratings.\nAfter graphing both with a sample of 50 highest NumVotes, the graph strategy pointed to a better data visual, a clear trend. The results where visible following a trend where the higher the rating the higher the numVotes. #Unfortunatley strategy one was picking up multiple outliers, and could not base a recomendation on outliers only; a trend was not clearly visible with strategy 1.\nCombining both together. I have identified as movies &gt; 5 ratings with 2M NumVotes to be top performing movies. And movies at &lt; 5 ratings with &lt;25K numVotes as worst performers [for a shorter list of worst performers- low ratings with high numVotes will give me worst performers].\nFINAL SUCESS CODE AND PLOT\n\nFinal_sucess &lt;- full_join(TITLE_BASICS,TITLE_RATINGS, by = \"tconst\") |&gt; \n  select(tconst, titleType, primaryTitle, startYear,genres, averageRating,numVotes) |&gt; \n  filter(titleType == \"movie\", averageRating &gt; 5, numVotes &gt; 2000000) |&gt; \n  group_by(primaryTitle) |&gt; \n  arrange(desc(numVotes)) \n\nplot_final = head(Final_sucess, 50) \n\nggplot(plot_final, aes(x=averageRating, y=numVotes))  +\n  geom_point()\n\n\n\n\n\n\n\n\nFINAL SUCESS NOT CODE AND PLOT\n\nFinal_sucess_NOT &lt;- full_join(TITLE_BASICS,TITLE_RATINGS, by = \"tconst\") |&gt; \n  select(tconst, titleType, primaryTitle, startYear,genres, averageRating,numVotes) |&gt; \n  filter(titleType == \"movie\", averageRating &lt; 5, numVotes &lt; 25000) |&gt; \n  group_by(primaryTitle) |&gt; \n  arrange(desc(numVotes)) \n\nplot_final_not = head(Final_sucess_NOT, 50) \n\nggplot(plot_final_not, aes(x=averageRating, y=numVotes)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\nWhat was the genre with the most “successes” in each decade?\n\nsucess_decade &lt;- full_join(TITLE_BASICS,TITLE_RATINGS, by = \"tconst\") |&gt; \n  select(tconst, titleType, primaryTitle, startYear,genres, averageRating,numVotes) |&gt; \n  filter(titleType == \"movie\", averageRating &gt; 5, numVotes &gt; 1000000) |&gt; \n  group_by(primaryTitle) |&gt; \n  arrange(desc(numVotes)) |&gt; \n  separate_longer_delim(genres, \",\") \n\nplt = head(sucess_decade, 500)\n\nggplot(plt, aes(startYear, averageRating)) + \n  geom_point(aes(colour = factor(genres)))\n\n\n\n\n\n\n\n\nWhat genre consistently has the most “successes”? What genre used to reliably produced “successes” and has fallen out of favor?\nFrom 1970-2010 most successful genres appear to be drama & crime.\nWhat genre has produced the most “successes” since 2010? Does it have the highest success rate or does it only have a large number of successes because there are many productions in that genre?\nFrom 2010-2020 Sci-Fi had a tick up - success, at ratings higher than 8.5.\nWhat genre has become more popular in recent years?\nSci-Fi and Thrillers have risen in favor in most recent year\n\n\n\nIdentify (at least) two actors and one director who you will target as the key talent for your movie. Write a short “pitch” as to why they are likely to be successful. You should support your pitch with at least one graphic and one table.\nANSWER : Actor = Stephen King, for his popular movie ‘The Shawshank Redemption’ + Actress = Gundula Janowitz, for her popular movie ‘The Shawshank Redemption’.\n\nNAME_BASICS &lt;- NAME_BASICS |&gt;\n  rename_at('knownForTitles', ~'tconst')\n\nactors &lt;- NAME_BASICS |&gt; \n  separate_longer_delim(primaryProfession, \",\") |&gt; \n  separate_longer_delim(tconst, \",\") |&gt; \n  filter (primaryProfession == 'actor') |&gt; \n  select (tconst, primaryProfession, primaryName, birthYear, deathYear)\n\nactress &lt;- NAME_BASICS |&gt; \n  separate_longer_delim(primaryProfession, \",\") |&gt; \n  separate_longer_delim(tconst, \",\") |&gt; \n  filter (primaryProfession == 'actress') |&gt; \n  select (tconst, primaryProfession, primaryName, birthYear, deathYear)\n\nmovies &lt;- TITLE_BASICS |&gt; \n  separate_longer_delim(genres, \",\") |&gt; \n  separate_longer_delim(tconst, \",\") |&gt; \n  filter(titleType == 'movie') |&gt; \n  select(tconst, titleType, primaryTitle,genres)\n\nmovie_actor &lt;- full_join(actors,movies, by = 'tconst')\n\nWarning in full_join(actors, movies, by = \"tconst\"): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 1 of `x` matches multiple rows in `y`.\nℹ Row 11806 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\nmovie_actress &lt;- full_join(actress,movies, by = 'tconst')\n\nWarning in full_join(actress, movies, by = \"tconst\"): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 1 of `x` matches multiple rows in `y`.\nℹ Row 29522 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\npopular &lt;- TITLE_RATINGS |&gt; \n  separate_longer_delim(tconst, \",\")\n\npopular_actor_sucess &lt;- full_join(popular,movie_actor, by = 'tconst') |&gt; \n  filter(averageRating &gt; 5, numVotes &gt; 1000000) |&gt; \n  arrange(desc(numVotes)) |&gt; \n  slice_head(n=50) \n\npopular_actress_sucess &lt;- full_join(popular,movie_actress, by = 'tconst') |&gt; \n  filter(averageRating &gt; 5, numVotes &gt; 1000000) |&gt; \n  arrange(desc(numVotes)) |&gt; \n  slice_head(n=50) \n\n\n\n\nFind a classic movie to remake with your key talent. The original should have a large number of IMDb ratings, a high average rating, and not have been remade in the past 25 years.4\nOnce you have found your classic movie to remake, confirm whether key actors, directors, or writers from the original are still alive. If so, you need to contact your legal department to ensure they can secure the rights to the project. You may also want to include the classic actors as “fan service.”\nANSWER: I’m not a fan of remakes, so I would propose an entirely new movie based on the hit ‘The Shawshank Redemption’ - at the time it high very high ratings and votes. Give the people what they like. As a fan of Morgan Freeman and Stephen King, I would propose King write a new drama/thriller/inspirational, and Freeman star and produce it. Perfect combo and still alive doing great content. My team will contact them.\n\n\n\nIn today’s environment, the pattern has been to remake and create movies that resemble a ‘passing the torch’ feeling.\nTake for example a classic Hocus Pocus, or GhostBusters, or recet Bettle Juice, a recreation of the old. It start to look like a series and not a movie.\nWhy not create an entirely new generation of movies, to both inspire the new generations while at the same time rekindle the nostalgia of the older generations.\nI propose a making of a new type of DRAMA, DRAMA101 - Where our movies will contain multiple genres, from drama, thriller [for oldies], inspirational [for sensitivity of today’s environment], touch of comedy [reflecting real life]. A mix. New.\nWhy not reflect the different stages/ feelings in one movie?\n“Inside Out” you may be thinking…“childish” you say…\nNO…rather this is reality, a movie should reflect a reality of life. A new Genre.\nI propose to create a movie similar to the greatest hit ‘The Shawshank Redemption’, Inspired by Stephen King’s book “Rita Hayworth and Shawshank Redemption”, Produced and stared by Morgan Freeman , one of the greatest inspirational person in media now.\nThe movie could follow a similar plot as Stephen King book to start, innocent man/woman in jail, finds purpose in life. But instead of a killing it could be another type of crime, like crossing a border, or robbing to survive, or falsely accusations.\nThe name for the movie could be “Innocent until proven Guilty” narrated by Morgan Freeman and Edith Mathis. Stared by a fresh set of new diverse actors, never seen actors - the normal everyday person as an actor. Are you ready to bring the next genre of the movie future.\n‘Out with the old and in with the new.’"
  },
  {
    "objectID": "mp03.html",
    "href": "mp03.html",
    "title": "Mini Project 3 by Maria Alexandra Jerez",
    "section": "",
    "text": "U.S. ELECTIONS\n\n\n\nINTRO\nFor this project we will be diving into politics and voter allocations. We have a snap of magnificent data on politics, and boy will you be blown away. Let’s dive into it….\nWe will start with opening all libraries needed and creating the main directory, sub-directory, my directory, and a folder to store data files. Here is the code we will use to accomplish this:\n\n\nlibrary(tidyverse) # includes: ggplot2, tidyr, dplyr, readr, purrr, tibble, stringr, forcats \nlibrary(utils)\nlibrary(sf)\nlibrary(httr) \nlibrary(fs)\nlibrary(plyr)\n\n#define main directory\n  main_dir &lt;- \"/Users/mariajerez/Documents/GitHub\"\n  \n#define sub directory\n  sub_dir &lt;- \"STA9750-2024-FALL\"\n  \n#define directory\n  my_directory &lt;- file.path(main_dir, sub_dir)\n  \n\nFor our first Task, we will be downloading a set of files. Here is the code we will use to accomplish the task:\n\n\n# create a house data file - file dowloaded manually and saved to directory \nsetwd(my_directory)  \nHouse &lt;- read.csv(\"1976_2022_house.csv\")\n  \n# create a president data file - file dowloaded manually and saved to directory \n  President &lt;- read.csv(\"1976_2020_president.csv\")\n  \n#Download files districts94-112 - auto load a .zip file from directly from the URL\n  download_zip_files_CD &lt;- \n      download.file(\"https://cdmaps.polisci.ucla.edu/shp/districts094.zip\", \n                destfile = \"districts094.zip\" , mode='wb')\n\nFor our second task, we will be auto downloading congressional shapefiles from Census Beaurue. Here is the code used to accomplished this task:\n\n\nInitial Exploration of Vote Count Data\nFor our third task we will be exploring the vote count data and answering a few questions below using the [MIT Election Data Science Lab] (https://electionlab.mit.edu/) data sources.\n\nWhich states have gained and lost the most seats in the US House of Representatives between 1976 and 2022? [House data source](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/IG0UN2)\n\nIf we take the argument that an increase in voters increases house seats. Then we can conclude, from our data, that no state has lost seats from 1976-2022, as total votes for each state has increased over time. Below we will see a graph of such results for all states, with the exception of West Virginia, where we see a stale pattern through the years.\n\nprint(house_seats)\n\n# A tibble: 32,452 × 6\n# Groups:   year, state [1,201]\n    year state   party         candidate                 district candidatevotes\n   &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;         &lt;chr&gt;                        &lt;int&gt;          &lt;int&gt;\n 1  1976 ALABAMA \"DEMOCRAT\"    \"BILL DAVENPORT\"                 1          58906\n 2  1976 ALABAMA \"REPUBLICAN\"  \"JACK EDWARDS\"                   1          98257\n 3  1976 ALABAMA \"\"            \"WRITEIN\"                        1              7\n 4  1976 ALABAMA \"DEMOCRAT\"    \"J CAROLE KEAHEY\"                2          66288\n 5  1976 ALABAMA \"REPUBLICAN\"  \"WILLIAM L \\\"BILL\\\" DICK…        2          90069\n 6  1976 ALABAMA \"\"            \"WRITEIN\"                        2              5\n 7  1976 ALABAMA \"DEMOCRAT\"    \"BILL NICHOLS\"                   3         106935\n 8  1976 ALABAMA \"PROHIBITION\" \"OGBURN GARDNER\"                 3           1111\n 9  1976 ALABAMA \"\"            \"WRITEIN\"                        3              2\n10  1976 ALABAMA \"REPUBLICAN\"  \"LEONARD WILSON\"                 4          34531\n# ℹ 32,442 more rows\n\n\nLet’s look at the following scenario for question 2:\n“New York State has a unique “fusion” voting system where one candidate can appear on multiple “lines” on the ballot and their vote counts are totaled. For instance, in 2022, Jerrold Nadler appeared on both the Democrat and Working Families party lines for NYS’ 12th Congressional District. He received 200,890 votes total (184,872 as a Democrat and 16,018 as WFP), easily defeating Michael Zumbluskas, who received 44,173 votes across three party lines (Republican, Conservative, and Parent). “\n\nAre there any elections in our data where the election would have had a different outcome if the “fusion” system was not used and candidates only received the votes their received from their “major party line” (Democrat or Republican) and not their total number of votes across all lines?\n\nBased on the data, New York and Connecticut follow a “fusion” system. For this analysis, if the ‘percent of total votes’ for each candidate is &gt; 50% on that year, than we can conclude that candidate as the winner. However, we will look closer at candidates that have won with a margin of +-.5% points from 50% ,[according to AP winner call](https://www.ap.org/elections/our-role/how-we-call-races/) +-.5% margin is considered ’too close to call”; too small a difference to win, and because a candidate won under a “fusion” system (votes for representation of 2 or more parties on ballot), we can conclude that there could have been a possibility for the opposing candidate to win if only one party’s votes (not multiples) was counted for the winner.\nThe table extracted from code below, provides us a list of candidates winning at 50%+, from which we look at the 51% winners as their vote count ranges from 50.5-51%, rounded to 51%; to close to call.\n\n\n#WINNER IF VOTES &gt; 50% \n  house_candidate_fusion &lt;- House |&gt;\n    filter(fusion_ticket == \"TRUE\", !is.na(party),!is.na(candidate), !is.na(year) ) |&gt; \n    group_by(year, candidate)|&gt;\n    mutate(percentoftotalvotes = round((candidatevotes/totalvotes*100), 2)) |&gt;\n    select(year, candidate, percentoftotalvotes, party) |&gt;\n    arrange(year) \n\n\n\n# TABLE\nlibrary(DT)\ndatatable(house_candidate_fusion, caption = \"Fusion Votes by Candidate and Year\")\n\n\n\n\n\nFor our fourth task we will be creating a code to automatically extract a ‘.shp’ file from a zip folder and reading to r. The code below will accomplish this task:\n\nVisualization of the 2000 Presidential Election Electoral College Results\nFor our fifth task we will now look at the winner for the 2000 elections.\n\n  Winner2000_plot &lt;- ggplot(Shape_2000, aes(geometry = geometry, fill = party_simplified),\n                     color = \"black\") +\n    geom_sf() + \n    scale_fill_manual(values = c(\"REPUBLICAN\" = \"firebrick2\", \"DEMOCRAT\" = \"royalblue2\")) +\n    theme_minimal() +\n    coord_sf(xlim = c(-180, -50), ylim = c(10,80), expand = FALSE) +\n    labs(title = \"2000 Presidential Election\", fill = \"WINNING Party\")\n \n\n\nprint(Winner2000_plot)\n\n\n\n\n\n\n\n\n\n\nAdvanced Chloropleth Visualization of Electoral College Results\nFor our sixth task we will look at a more advanced map of the results, below:\n\n#computer does not support running this - to much memory usage and can not run. \n#Merge the datasets\nShape_allyears &lt;- combined_shapefile_cd %&gt;%\n  left_join(winner_allyears, by = \"state\", relationship = \"many-to-many\")  # Join on the 'state' column\n\n# Inspect the merged dataset\nhead(Shape_allyears, n = 1)\n\n# Plot the election results over time\nlibrary(ggplot2)\nlibrary(sf)  # 'combined_shapefile_cd' is an sf object\n\n# Plotting the faceted map\nShape_allyears_plot  &lt;- ggplot(data = Shape_allyears) +\n  geom_sf(aes(geometry = geometry, fill = party_simplified), color = \"black\", size = 0.1) +\n  facet_wrap(~year) +\n  scale_fill_manual(values = c( \"DEMOCRAT\" = \"blue\", \"REPUBLICAN\" = \"red\")) +  # Customize colors for parties\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  labs(title = \"U.S. Election Results by State (1976-2020)\",\n       fill = \"Party\",\n       subtitle = \"Faceted by Year\")\n\nprint(Shape_allyears_plot)\n\n#computer memory is bad, have to save file instead of viewing \nggsave(\"election_results_map.png\", Shape_allyears_plot, width = 12, height = 8, dpi = 300)\n\n\n\nEffects of ECV Allocation Rules\nWe will now look at some approaches for Electoral College Vote distribution:\n\nState-Wide Winner-Take-All\nFor “Winner-take-all” we took the “U.S. President data 1976-2020” from Harvard database and compared to an actual_winner created excel file [researched president Winner by year, by state from 1976-2020]. We extracted the winning party and candidate from the Harvard data, by pulling the max/winning votes for each state, year; in other words, the candidate with most votes a state for a specific year takes all winning Electoral College Votes. Similarly, we take researched winner_actual data, to compare, which gives us the candidate and party that actually won. Hence, everything highlighted in green means, YES, the actual and analyzed winning party reflects a “Winner-take-all” approach. We do however have one outlier in 1980, New York, where max votes point to the opposite party as winner, hence not following a winner take all.\n\n#Data WINNING PARTY by year and state\nwinner_allyears &lt;- President %&gt;%\n  filter(year &gt;= 1976 & year &lt;= 2020) %&gt;%\n  group_by(year, state) %&gt;%\n  top_n(1, candidatevotes) %&gt;%\n  select(year, state, candidate, candidatevotes, party_simplified) %&gt;%\n  mutate(state = toupper(trimws(state)))\n\n#Actual winner data \nlibrary(\"readxl\")\nactual_winner &lt;- read_excel(\"/Users/mariajerez/Documents/GitHub/STA9750-2024-FALL/Winning Party by State_Original.xls\") |&gt;\n  mutate(year = as.integer(year)) \n\n#Standarize state bacause one is upper case and other is lower case \nactual_winner$state &lt;- toupper(actual_winner$state)\nwinner_allyears$state &lt;- toupper(winner_allyears$state)\n\n# Merge by both 'year' and 'state'\nresult &lt;- merge(actual_winner, winner_allyears, by = c('year', 'state'))\n\n#identify matching/unmatching\nresult &lt;- result %&gt;%\n  mutate(party_match = ifelse(party_simplified == `president winning party_actual`, \"Matched\", \"Unmatched\"))\n\nlibrary(DT)\nlibrary(dplyr)\n\n# Create the DataTable with color styling\nplot1 &lt;- datatable(result, caption = \"DISTRICT WIDE WINNER\") %&gt;%\n  formatStyle(\n    'party_match', \n    target = 'row',\n    backgroundColor = styleEqual(c('Matched', 'Unmatched'), c('lightgreen', 'lightcoral'))\n)\n\nplot1\n\n\n\n\n\nDistrict-Wide Winner-Take-All + State-Wide “At Large” Votes\nSimilarly to #1 of ‘Winner take-all’ we will expand to a district level. From the table, anything in red is no match, which means opposite party of actual winner should have won; Green = fair allocation of votes based on max votes per district, Red = unfair allocation votes. In other words, the Harvard data shows ‘candidate.x’ and ‘party’ as winner based on higher votes. Meanwhile the ‘actual_winner’ opposes or matches that argumet depicted as ‘candidate.y’ and ‘party_simplified’.\n\n\n#District-Wide Winner-Take-All + State-Wide “At Large” Votes\n#Winner by district based on house data\nallyears_district &lt;- House %&gt;%\n  filter(year &gt;= 1976 & year &lt;= 2020) %&gt;%\n  filter(year %in% c(1976, 1980, 1984, 1988, 1992, 1996, 2000, 2004, 2008, 2012, 2016, 2020)) %&gt;%\n  group_by(year, state_fips, district, state) %&gt;%\n  distinct(district, .keep_all = TRUE) %&gt;%\n  select(year, state_fips, district, state, candidate, party) %&gt;%\n  mutate(state = toupper(trimws(state)))\n\n\n\nwinner_allyears_president &lt;- President %&gt;%\n  filter(year &gt;= 1976 & year &lt;= 2020) %&gt;%\n  group_by(year, state_fips, state, candidate) %&gt;%\n  top_n(1, candidatevotes) %&gt;%\n  select(year, state_fips, state, candidate, candidatevotes, party_simplified) %&gt;%\n  mutate(state = toupper(trimws(state)))\n\n\n\n# Filter winner_allyears_president to get top candidate for each (year, state_fips)\nwinner_allyears_president_top &lt;- winner_allyears_president %&gt;%\n  group_by(year, state_fips) %&gt;%\n  top_n(1, candidatevotes) %&gt;%\n  ungroup()\n\n\n# Perform the join with distinct district\ncombined_data &lt;- allyears_district %&gt;%\n  left_join(winner_allyears_president_top, by = c(\"year\", \"state_fips\")) %&gt;%\n  distinct(district, .keep_all = TRUE) %&gt;%\n  mutate(state = toupper(trimws(state.x))) %&gt;%\n  select(state_fips, year, state, district, candidate.x, party, candidate.y, party_simplified)\n\n\n\nlibrary(DT)\nlibrary(dplyr)\n\n# Sample data (replace with your actual combined_data)\nresult2 &lt;- combined_data %&gt;%\n  mutate(party_match = ifelse(party_simplified == party, \"Matched\", \"Unmatched\"))\n\n# Create the DataTable with color styling\nplot &lt;- datatable(result2, caption = \"DISTRICT WIDE WINNER\") %&gt;%\n  formatStyle(\n    'party_match', \n    target = 'row',\n    backgroundColor = styleEqual(c('Matched', 'Unmatched'), c('lightgreen', 'lightcoral'))\n  )\n\nplot\n\n\n\n\n\n\nThe more granular we take MAX votes, the more unfavorable it is for a candidate. The data supports strongly a ‘WINNER TAKE-ALL’ approach. Should the electoral system follow different approaches, more factors must be considered, such as third + parties running, in contrast to only two parties running. The fear may lie in the disadvantages this would bring to candidates [top candidates] should a third or more parties come in, specially in a polarized election.\n\n\nCONCLUSION\nTo conclude, unless the majority of the citizen voters/ the people are aware of how elections are decided, we can not call an approach as fair, although, ‘YES’ , 100% it is a democracy. Still, multiple questions will arise.. whether Electoral college votes are fairly allocated? Do states have their fair share of ECV’s ? based on what? population? How about fusion votes? Why can one candidate represent multiple parties? are there not enough leaders aspiring for higher office roles to not be able to represent a party used by a fuse vote?\nIndeed, elections look quite complicated and they are often polarizing. However, in order to preserve a peaceful democracy perhaps educating the citizen voter is a great first step. Educating them as early as elementary school, because our voting system, our DEMOCRACY will be there for the rest of our lives. Not only that but our DEMOCRACY will determine our lifestyle. For the short term, perhaps a questions in the voting ballot of how we want ECV’s to be allocated would help."
  },
  {
    "objectID": "mp04.html",
    "href": "mp04.html",
    "title": "mp04",
    "section": "",
    "text": "Project 4 : Monte Carlo-Informed Selection of CUNY Retirement Plans\nby Maria Jerez\n\n\n\nMonte Carlo definition by Investopedia\n\n\nGREAT NEWS! I just got hired at Baruch college for the position as an ESOL professor. I’m at a difficult point, where I must choose one of two retirement plans offered : TRS & ORP. TRS is a pension plan, and ORP is like a 401K. To help me with my decision I will be analyzing a few metrics and sets of data, see below. Dive in and help me on this hard decision. I’ll take you through my analysis process and decision.\n\nData sources:\nFederal Reserve Bank of Saint Louis : US Economic data API\nMetrics:\n\nCPI as Inflation Data\nInternational Stock market as International Equity Market total returns data\nUS Monthly Treasury Yields as Short Term Debt Returns\n\nAlpha Vantage : free financial market data API\nMetrics:\n\nS&P 500 as US equity data\n10 Treasury Yield as Bond Market total returns data\nReal GDP as Wage Growth data\n\nI’ve requested a set of keys from the sources in order to extract data from it’s APIs. After completing these tasks, I then extracted and created data frames for the metrics above and analyzed each. See code below for code of extraction.\nMy keys:\n\n\nCLICK TO VIEW CODE FOR EXTRACTING AND CREATING DATA FRAME FOR ALPHA DATA\n\n&lt;/summary&gt;\n``` r\n\nlibrary(tidyverse)\nlibrary(httr2)\n\n# FRED OBSERVATIONS DATA - INFLATION \n# Parameters and request\nfred_url &lt;- \"https://api.stlouisfed.org/fred/series/observations\"\nfred_params &lt;- list(\n  series_id = \"CPIAUCSL\",    # Series ID = consumer price index for urban consumers/ Inflation data\n  api_key = fred_api_key,    # API key\n  file_type = \"json\",        # Specify response format as JSON\n  start_date = \"2000-01-01\", # Optional start date for data\n  end_date = \"2020-01-01\"    # Optional end date for data\n)\n\n# Send the GET request for series information\nfred_response &lt;- request(fred_url) |&gt;\n  req_url_query(!!!fred_params) |&gt;  # Pass query parameters\n  req_perform()\n\nfred_data &lt;- resp_body_json(fred_response)\n\n# Extract the observations data from the response\nobservations &lt;- fred_data$observations\n\n# Convert it to a data frame for easier analysis\nobservations_df &lt;- data.frame(\n  date = sapply(observations, function(x) x$date),\n  value = sapply(observations, function(x) x$value)\n)\n\n# FRED OBSERVATIONS DATA - IR14270 (if available, for international stock market data), values are lin= actual price/actual value\n# Parameters and request\nfred_url &lt;- \"https://api.stlouisfed.org/fred/series/observations\"\nfred_int_params &lt;- list(\n  series_id = \"IR14270\",     # Series ID = International stock market\n  api_key = fred_api_key,    # API key\n  file_type = \"json\",        # Specify response format as JSON\n  start_date = \"2000-01-01\", # Optional start date for data\n  end_date = \"2020-01-01\"    # Optional end date for data\n)\n\n# Send the GET request for series information\nfred_int_response &lt;- request(fred_url) |&gt;\n  req_url_query(!!!fred_int_params) |&gt;  # Pass query parameters\n  req_perform()\n\nfred_int_data &lt;- resp_body_json(fred_int_response)\n\n\n# Extract the observations data from the response\nobservations_int &lt;- fred_int_data$observations\n\n# Convert it to a data frame for easier analysis\nobservations_int_df &lt;- data.frame(\n  date = sapply(observations_int, function(x) x$date),\n  value = sapply(observations_int, function(x) x$value)\n)\n\n\n####### SHORT TERM DEBT RETURS [STDR] - USE MONTHLY TREASURY YIELDS with units= lin[percentage]\n# Parameters and request\nfred_url &lt;- \"https://api.stlouisfed.org/fred/series/observations\"\nfred_stdr_params &lt;- list(\n  series_id = \"DGS1MO\",     # Series ID = Treasury Yiields\n  api_key = fred_api_key,    # API key\n  file_type = \"json\",        # Specify response format as JSON\n  start_date = \"2000-01-01\", # Optional start date for data\n  end_date = \"2020-01-01\"    # Optional end date for data\n)\n\n# Send the GET request for series information\nfred_stdr_response &lt;- request(fred_url) |&gt;\n  req_url_query(!!!fred_stdr_params) |&gt;  # Pass query parameters\n  req_perform()\n\nfred_stdr_data &lt;- resp_body_json(fred_stdr_response)\n\n\n# Extract the observations data from the response\nobservations_stdr &lt;- fred_stdr_data$observations\n\n# Convert it to a data frame for easier analysis\nobservations_stdr_df &lt;- data.frame(\n  date = sapply(observations_stdr, function(x) x$date),\n  value = sapply(observations_stdr, function(x) x$value)\n)\n\n```\n\n\n\nCLICK TO VIEW CODE FOR EXTRACTING AND CREATING DATA FRAME FOR ALPHA DATA\n\n```r\n\nlibrary(tidyverse)\nlibrary(httr2)\n\n#ALPHAVANTAGE DATA - US EQUITY \n\nreq &lt;- request(\"https://www.alphavantage.co/query\") |&gt;\n  req_url_query(`function` = \"TIME_SERIES_MONTHLY\", \n                symbol = \"SPY\",  #s%p 500, US equity data \n                apikey = API_KEY) |&gt;\n  req_perform() \n\n#JSON response\nsp_data &lt;- resp_body_json(req)\n\n# Extract the time series data (the 'Time Series (Monthly)' part)\nsp_time_series &lt;- sp_data[[\"Monthly Time Series\"]] \n\n# Convert it into a data frame\nsp_time_series_df &lt;- data.frame(\n  date = as.Date(names(sp_time_series)),  # Convert dates to Date class\n  open = sapply(sp_time_series, function(x) as.numeric(x[[\"1. open\"]])),\n  high = sapply(sp_time_series, function(x) as.numeric(x[[\"2. high\"]])),\n  low = sapply(sp_time_series, function(x) as.numeric(x[[\"3. low\"]])),\n  close = sapply(sp_time_series, function(x) as.numeric(x[[\"4. close\"]])),\n  volume = sapply(sp_time_series, function(x) as.numeric(x[[\"5. volume\"]])),\n  stringsAsFactors = FALSE  # Avoid factors\n)\n\n\n#### ALPHAVANTAGE DATA - 10 YEAR TREASURY BOND YIELD in percent \n\nbond_req &lt;- request(\"https://www.alphavantage.co/query\") |&gt;\nreq_url_query(`function` = \"TREASURY_YIELD\", \n              interval = \"monthly\",  \n              apikey = API_KEY) |&gt;\n  req_perform() \n\n#JSON response\nbond_data &lt;- resp_body_json(bond_req)\n\n\n# Extract and process the bond data\nbond_df_data &lt;- bond_data$data \n\n# Extract 'date' and 'value' from bond_df_data\ndates &lt;- sapply(bond_df_data, function(x) x$date)  # Extract 'date'\nvalues &lt;- sapply(bond_df_data, function(x) as.numeric(x$value))  # Extract 'value' and convert to numeric\n\n# Create the data frame\nbond_df &lt;- data.frame(\n  date = as.Date(dates),  # Convert 'date' to Date class\n  value = values,  # Numeric 'value'\n  stringsAsFactors = FALSE  # Avoid factors\n)\n\n\n### REAL GDP, as GDP INCREASES SO DOES WAGE GROWTH in Billions of dollars \n#https://www.alphavantage.co/query?function=REAL_GDP&interval=annual&apikey=demo\n\ngdp_req &lt;- request(\"https://www.alphavantage.co/query\") |&gt;\n  req_url_query(`function` = \"REAL_GDP\", \n                interval = \"annual\",  \n                apikey = API_KEY) |&gt;\n  req_perform() \n\n#JSON response\ngdp_data &lt;- resp_body_json(gdp_req)\n\n\n# Extract and process the bond data\ngdp_df_data &lt;- gdp_data$data \n\n# Extract 'date' and 'value' from bond_df_data\ndates &lt;- sapply(gdp_df_data, function(x) x$date)  # Extract 'date'\nvalues &lt;- sapply(gdp_df_data, function(x) as.numeric(x$value))  # Extract 'value' and convert to numeric\n\n# Create the data frame\ngdp_df &lt;- data.frame(\n  date = as.Date(dates),  # Convert 'date' to Date class\n  value = values,  # Numeric 'value'\n  stringsAsFactors = FALSE  # Avoid factors\n)\n\n```\n\n\n\nEconomy Analysis\nI will now dive into my initial economic analysis based on the metrics described at the beginning. I’m looking for trends that stand out. Here are some findings I came across and graphs to help us better understand.\n\nS&P 500 as US equity data\n\n\nCLICK TO VIEW CODE FOR GRAPH S&P500_US EQUITY - Monthly Average High and Low Values\n\n```r\n\nreq &lt;- request(\"https://www.alphavantage.co/query\") |&gt;\n  req_url_query(`function` = \"TIME_SERIES_MONTHLY\", \n                symbol = \"SPY\",  #s%p 500, US equity data \n                apikey = API_KEY) |&gt;\n  req_perform() \n\n#JSON response\ndata &lt;- resp_body_json(req)\n\n#Inspect the structure of the data\nstr(data)\n\n# Extract the time series data (the 'Time Series (Monthly)' part)\ntime_series &lt;- data[[\"Monthly Time Series\"]] \n\n# Convert it into a data frame\ntime_series_df &lt;- data.frame(\n  date = as.Date(names(time_series)),  # Convert dates to Date class\n  open = sapply(time_series, function(x) as.numeric(x[[\"1. open\"]])),\n  high = sapply(time_series, function(x) as.numeric(x[[\"2. high\"]])),\n  low = sapply(time_series, function(x) as.numeric(x[[\"3. low\"]])),\n  close = sapply(time_series, function(x) as.numeric(x[[\"4. close\"]])),\n  volume = sapply(time_series, function(x) as.numeric(x[[\"5. volume\"]])),\n  stringsAsFactors = FALSE  # Avoid factors\n)\n\n# View the first few rows of the data\nstr(time_series_df)\n\n# Group by month (and possibly year if you want to avoid year-based aggregation)\nmonthly_avg_df &lt;- time_series_df |&gt;\n  mutate(month = floor_date(date, \"month\")) |&gt;  # Round the date to the start of the month\n  group_by(month) |&gt;  # Group by month\n  summarise(\n    avg_open = mean(open, na.rm = TRUE),\n    avg_high = mean(high, na.rm = TRUE),\n    avg_low = mean(low, na.rm = TRUE),\n    avg_close = mean(close, na.rm = TRUE),\n    avg_volume = mean(volume, na.rm = TRUE)\n  )\n\n# View the first few rows of the monthly averages\nhead(monthly_avg_df)\n\n# Reshape the data to long format for ggplot\nmonthly_avg_high_low &lt;- monthly_avg_df |&gt;\n  select(month, avg_high, avg_low) |&gt;\n  gather(key = \"metric\", value = \"value\", -month)\n\nglimpse(monthly_avg_high_low)\n\n# Plot the monthly high and low averages over time\nssplot &lt;- ggplot(monthly_avg_high_low, aes(x = month, y = value, color = metric)) +\n  geom_line(size = .10) +  # Line plot for high and low values\n  geom_point(size = .25) +  # Points to mark data points\n  geom_smooth(method = \"lm\", color = \"black\", linetype = \"dashed\", size = .05) +\n  labs(\n    title = \"S&P500_US EQUITY - Monthly Average High and Low Values\",\n    x = \"Month\",\n    y = \"Average $ Value \",\n    color = \"Metric\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n```\n\n\n\n\nAlt text\n\n\nI am using the S&P 500 to portray US Equity. The S&P 500 index reflects the 500 largest publicly traded companies in the US., spanning from industries such as healthcare, finance, consumer goods, and energy. I can conclude that the better the S&P performs the better it reflects US equity. In other words an upward trend = growth, downward trend = difficulties.\nFrom this graph, in a spam of 20+ years, we have a healthy upward trend, we have growth. However, zooming into chunks of years, particularly in recent years there seems to be a steeper spike upwards, perhaps too healthy a market. As we also see from the graph, after a spike comes a drop. For example zooming into years 2007-2008sh, a spike happened around 2007 when suddenly dropped in 2008. Of course we must consider historical events, in 2008 the stock market crashed due to a housing bubble. As another example, we see another big slope up and then down from 2020-2021, the historical event for this was Covid-19.\nTo conclude on this graph, US equity looks fantastico in a longer time span and bad on a short time span. But increasing either way.\n\n\n10 Treasury Yield as Bond Market total returns data\n\n\nCLICK TO VIEW CODE FOR GRAPH 10 Year Treasury Bond Yield; Bond Market Returns - Monthly Average % values\n\n```r\n\nbond_req &lt;- request(\"https://www.alphavantage.co/query\") |&gt;\nreq_url_query(`function` = \"TREASURY_YIELD\", \n              interval = \"monthly\",  \n              apikey = API_KEY) |&gt;\n  req_perform() \n\n#JSON response\nbond_data &lt;- resp_body_json(bond_req)\n\n\n# Extract and process the bond data\nbond_df_data &lt;- bond_data$data \n\n# Extract 'date' and 'value' from bond_df_data\ndates &lt;- sapply(bond_df_data, function(x) x$date)  # Extract 'date'\nvalues &lt;- sapply(bond_df_data, function(x) as.numeric(x$value))  # Extract 'value' and convert to numeric\n\n# Create the data frame\nbond_df &lt;- data.frame(\n  date = as.Date(dates),  # Convert 'date' to Date class\n  value = values,  # Numeric 'value'\n  stringsAsFactors = FALSE  # Avoid factors\n)\n\n\n# Group by month (and possibly year if you want to avoid year-based aggregation)\nmonthly_bondavg_df &lt;- bond_df |&gt;\n  mutate(month = floor_date(date, \"month\")) |&gt;  # Round the date to the start of the month\n  group_by(month) |&gt;  # Group by month\n  summarise(\n    avg_value = mean(value, na.rm = TRUE)\n  )\n\n\n# Reshape the data to long format for ggplot\nmonthly_bondavg_plotdata &lt;- monthly_bondavg_df |&gt;\n  select(month, avg_value) |&gt;\n  gather(key = \"metric\", value = \"value\", -month)\n\n\n# Plot the monthly high and low averages over time\ntenyear_plot &lt;- ggplot(monthly_bondavg_plotdata, aes(x = month, y = value, color = metric)) +\n  geom_line(size = .10) +  # Line plot for high and low values\n  geom_point(size = .25) +  # Points to mark data points\n  labs(\n    title = \"10 Year Treasury Bond Yield; Bond Market Returns - Monthly Average % values\",\n    x = \"Month\",\n    y = \"Average % Value \",\n    color = \"Metric\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n```\n\n\n\n\nThis is an image\n\n\nFor this graph I have the 10 year Treasury Bond reflecting Bond Market returns over time. The 10 year Treasury Bond Yield is the percent interest paid by government for borrowed money for 10 years. The higher the % interest the more money payed back to investors/lenders the better. As it is government backed, it is considered a safe investment.\nFrom the graph we also see % rates rise up until 1980s and then fall. Rising rate means confidence in the economy, higher bond prices, higher returns. Falling rates mean low confidence in the economy, lower bond prices, lower returns.\nQuick note: The 1980’s represent an era of consumerism and materialism, as well as the birth of MTV! In the 1990s the internet takes the house.\nSo it looks like confidence for investors is growing from 2020 on, perhaps from higher returns from this bond investment. If history repeats itself and should the 10 year treasury bond reach a 10%+ yield such as that of the 1980s, than it may just be a good time to invest in this bond and wait till it reaches peak……or not? Caution and further research must be done.\n\n\nReal GDP as Wage Growth data\n\n\nCLICK TO VIEW CODE FOR GRAPH Real GDP; Wage Growth - Monthly Average Billion $s\n\n```r\n\ngdp_req &lt;- request(\"https://www.alphavantage.co/query\") |&gt;\n  req_url_query(`function` = \"REAL_GDP\", \n                interval = \"annual\",  \n                apikey = API_KEY) |&gt;\n  req_perform() \n\n#JSON response\ngdp_data &lt;- resp_body_json(gdp_req)\n\n\n# Extract and process the bond data\ngdp_df_data &lt;- gdp_data$data \n\n# Extract 'date' and 'value' from bond_df_data\ndates &lt;- sapply(gdp_df_data, function(x) x$date)  # Extract 'date'\nvalues &lt;- sapply(gdp_df_data, function(x) as.numeric(x$value))  # Extract 'value' and convert to numeric\n\n# Create the data frame\ngdp_df &lt;- data.frame(\n  date = as.Date(dates),  # Convert 'date' to Date class\n  value = values,  # Numeric 'value'\n  stringsAsFactors = FALSE  # Avoid factors\n)\n\n\n# Group by month (and possibly year if you want to avoid year-based aggregation)\nmonthly_gdpavg_df &lt;- gdp_df |&gt;\n  mutate(month = floor_date(date, \"month\")) |&gt;  # Round the date to the start of the month\n  group_by(month) |&gt;  # Group by month\n  summarise(\n    avg_value = mean(value, na.rm = TRUE)\n  )\n\n\n# Reshape the data to long format for ggplot\nmonthly_gdpavg_plotdata &lt;- monthly_gdpavg_df |&gt;\n  select(month, avg_value) |&gt;\n  gather(key = \"metric\", value = \"value\", -month)\n\n# Plot the monthly high and low averages over time\nrealgdp_plot &lt;- ggplot(monthly_gdpavg_plotdata, aes(x = month, y = value, color = metric)) +\n  geom_line(size = .10) +  # Line plot for high and low values\n  geom_point(size = .25) +  # Points to mark data points\n  geom_smooth(method = \"lm\", color = \"black\", linetype = \"dashed\", size = .2) + \n  labs(\n    title = \"Real GDP; Wage Growth - Monthly Average Billion $\",\n    x = \"Month\",\n    y = \"Average Billion $ \",\n    color = \"Metric\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n```\n\nFor this graph I use Real GDP as an indicator of Wage Growth. Real GDP is another indicator of the economy health, it measures the values of goods and services produced in a period of time. One conclusion I’ve made is that a rise in goods and services is a result of a rise in demand for goods, if effect then is a rise in labor to meet demand,and hence a rise in wages to fill labor gaps = wage growth.\nSo, I can safely say that because Real GDP has been rising , wages will also continue to rise. This also confirms that my wage will continue to increase over time, more money in my bag, more money to save! But how true is that? I must find a way to maintain the same expenses until I retire, frugality is key….maybe just maybe in order to preserve this trend and bag more cash in my pocket I will cut on some proteins and carbs when prices of goods rise. No more Turkey for me.\n\n\nCPI as Inflation Data\n\n\nCLICK TO VIEW CODE FOR GRAPH - Monthly CPI (Consumer Price Index) Over Time as INFLATION\n\n```r\n\n# Parameters and request\nfred_url &lt;- \"https://api.stlouisfed.org/fred/series/observations\"\nfred_params &lt;- list(\n  series_id = \"CPIAUCSL\",    # Series ID = consumer price index for urban consumers/ Inflation data\n  api_key = fred_api_key,    # API key\n  file_type = \"json\",        # Specify response format as JSON\n  start_date = \"2000-01-01\", # Optional start date for data\n  end_date = \"2020-01-01\"    # Optional end date for data\n)\n\n# Send the GET request for series information\nfred_response &lt;- request(fred_url) |&gt;\n  req_url_query(!!!fred_params) |&gt;  # Pass query parameters\n  req_perform()\n\nfred_data &lt;- resp_body_json(fred_response)\n\n\n# Extract the observations data from the response\nobservations &lt;- fred_data$observations\n\n# Convert it to a data frame for easier analysis\nobservations_df &lt;- data.frame(\n  date = sapply(observations, function(x) x$date),\n  value = sapply(observations, function(x) x$value)\n)\n\n\nobservations_df$date &lt;- as.Date(observations_df$date, format = \"%Y-%m-%d\")  # convert date chr to date\n\n# Downsample to monthly data by grouping and summarizing\n# We can summarize by taking the average for each month (or any other summary statistic)\nmonthly_data &lt;- observations_df |&gt;\n  mutate(month = floor_date(date, \"month\")) |&gt;  # Round the date down to the start of the month\n  group_by(month) |&gt;  # Group by the rounded month\n  summarise(monthly_value = mean(as.numeric(value), na.rm = TRUE))  # Summarize the values (e.g., average)\n\n\n# Plot the monthly data using ggplot2 with a trend line\ncpi_plot &lt;- ggplot(monthly_data, aes(x = month, y = monthly_value)) +\n  geom_line(color = \"blue\", size = .5) +              # Line plot\n  geom_point(color = \"red\", size = .05) +              # Points on the line\n  geom_smooth(method = \"lm\", color = \"black\", linetype = \"dashed\", size = .25) +  # Add a linear trend line\n  labs(title = \"Monthly CPI (Consumer Price Index) Over Time: INFLATION\",\n       x = \"Month\",\n       y = \"Average CPI Value\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for better readability\n\n```\n\n\n\n\n\n\n\n\n\n\nFor this graph I use CPI[Consumer price index] to evaluate inflation. Or should I call it Inflammation…because boy does this hurt, especially when it rises quickly such as what we have seen happen sometime after 2020. There goes my frugality from wage growth savings. Back to analysis. CPI is a measure of prices paid by consumers for services and goods. Hence the rise reflects the rise in expenses, the consumer is paying more for goods and services. I am paying more for rice with chicken!!\nTo conclude here, inflation will kill my savings. Perhaps if it falls below the trend line I’m relatively safe?…..\n\n\nInternational Stock market as International Equity Market total returns data\n\n\nCLICK TO VIEW CODE FOR GRAPH - Monthly International Stock Market Avg. Value Over Time\n\n```r\n\n# Parameters and request\nfred_url &lt;- \"https://api.stlouisfed.org/fred/series/observations\"\nfred_int_params &lt;- list(\n  series_id = \"IR14270\",     # Series ID = International stock market\n  api_key = fred_api_key,    # API key\n  file_type = \"json\",        # Specify response format as JSON\n  start_date = \"2000-01-01\", # Optional start date for data\n  end_date = \"2020-01-01\"    # Optional end date for data\n)\n\n# Send the GET request for series information\nfred_int_response &lt;- request(fred_url) |&gt;\n  req_url_query(!!!fred_int_params) |&gt;  # Pass query parameters\n  req_perform()\n\nfred_int_data &lt;- resp_body_json(fred_int_response)\n\n\n# Extract the observations data from the response\nobservations_int &lt;- fred_int_data$observations\n\n# Convert it to a data frame for easier analysis\nobservations_int_df &lt;- data.frame(\n  date = sapply(observations_int, function(x) x$date),\n  value = sapply(observations_int, function(x) x$value)\n)\n\n\nobservations_int_df$date &lt;- as.Date(observations_int_df$date, format = \"%Y-%m-%d\")  # convert date chr to date\n\n# Downsample to monthly data by grouping and summarizing\n# We can summarize by taking the average for each month (or any other summary statistic)\nmonthly_int_data &lt;- observations_int_df |&gt;\n  mutate(month = floor_date(date, \"month\")) |&gt;  # Round the date down to the start of the month\n  group_by(month) |&gt;  # Group by the rounded month\n  summarise(monthly_value = mean(as.numeric(value), na.rm = TRUE))  # Summarize the values (e.g., average)\n\n\n# Plot the monthly data using ggplot2 with a trend line\ninternationalstock_plot &lt;- ggplot(monthly_int_data, aes(x = month, y = monthly_value)) +\n  geom_line(color = \"green\", size = .2) +              # Line plot\n  geom_point(color = \"red\", size = .1) +              # Points on the line\n  geom_smooth(method = \"lm\", color = \"black\", linetype = \"dashed\", size = .025) +  # Add a linear trend line\n  labs(title = \"Monthly International Stock Market Avg. Value Over Time\",\n       x = \"Month\",\n       y = \"Average $ Value\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for better readability\n\n```\n\n\n\n\n\n\n\n\n\n\nFor this graph I look at International stock market data for International Equity analysis. The international stock market measure stocks that are international to the investors country of investment. In contrast to S&P 500 that measures US only equities.\nFrom the graph, the overall trend is rising, we do see time periods for grater equity; peaks; rises , such as those in 2010-2012sh and most recently 2023-2024sh.\nI will conclude that investing in international markets will diversify my investments and potentially level up any losses [if any] had by investing on S&P.\n\n\nUS Monthly Treasury Yields as Short Term Debt Returns\n\n\nCLICK TO VIEW CODE FOR GRAPH - US Monthly Treasury Yield % ; Short Term Debt Returns Over Time\n\n```r\n\n# Parameters and request\nfred_url &lt;- \"https://api.stlouisfed.org/fred/series/observations\"\nfred_stdr_params &lt;- list(\n  series_id = \"DGS1MO\",     # Series ID = Treasury Yields\n  api_key = fred_api_key,    # API key\n  file_type = \"json\",        # Specify response format as JSON\n  start_date = \"2000-01-01\", # Optional start date for data\n  end_date = \"2020-01-01\"    # Optional end date for data\n)\n\n# Send the GET request for series information\nfred_stdr_response &lt;- request(fred_url) |&gt;\n  req_url_query(!!!fred_stdr_params) |&gt;  # Pass query parameters\n  req_perform()\n\nfred_stdr_data &lt;- resp_body_json(fred_stdr_response)\n\n\n# Extract the observations data from the response\nobservations_stdr &lt;- fred_stdr_data$observations\n\n# Convert it to a data frame for easier analysis\nobservations_stdr_df &lt;- data.frame(\n  date = sapply(observations_stdr, function(x) x$date),\n  value = sapply(observations_stdr, function(x) x$value)\n)\n\nobservations_stdr_df$date &lt;- as.Date(observations_stdr_df$date, format = \"%Y-%m-%d\")  # convert date chr to date\n\n# Downsample to monthly data by grouping and summarizing\n# We can summarize by taking the average for each month (or any other summary statistic)\nmonthly_stdr_data &lt;- observations_stdr_df |&gt;\n  mutate(month = floor_date(date, \"month\")) |&gt;  # Round the date down to the start of the month\n  group_by(month) |&gt;  # Group by the rounded month\n  summarise(monthly_value = mean(as.numeric(value), na.rm = TRUE))  # Summarize the values (e.g., average)\n\n\n# Plot the monthly data using ggplot2 with a trend line\nmonthly_treasuryyield &lt;- ggplot(monthly_stdr_data, aes(x = month, y = monthly_value)) +\n  geom_line(color = \"green\", size = .2) +              # Line plot\n  geom_point(color = \"red\", size = .1) +              # Points on the line\n  geom_smooth(method = \"lm\", color = \"black\", linetype = \"dashed\", size = .05) +  # Add a linear trend line\n  labs(title = \"US Monthly Treasury Yield % ; Short Term Debt Returns Over Time\",\n       x = \"Month\",\n       y = \"Average % Values\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for better readability\n\n```\n\n\n\n\n\n\n\n\n\n\nFrom this graph I can conclude that this is like a heart beat EKG under stress. I am analyzing the US monthly treasury yield to view any short term debt return trends. The US monthly Treasury yield is like the 10 year yield, interest payed by government for borrowed money, but monthly. Again, it’s an indicator of economy health in a short term.\nAs % yields rise, so does short term debt returns. Meaning, more short term debts are payed-off/ returned with higher % rates. As % yields fall, so does short term debt returns. That makes sense, government wants to pay off higher rate loans, a money loss for them. However it looks like, from the graph, in a time period between 2007-2017sh, yields are relatively low and stagnant. The same happens sometime between 2020-2022. Is it stress? less borrowing?\nIn terms of favoring my retirement, maybe investing/ lending when rates are increasing will be a plus. I will be collecting greater interest, lets hope this is the trend when it’s time for me to retire.\n\n\n\nMonthly retirement returns from TRS & ORP\n\nTRS\nAfter Economy analysis I will now calculate monthly Benefits for the TRS plan and potential monthly benefits for ORP.\nI’ll First calculate TRS. I will keep in mind that TRS benefit is a pension benefit. When it’s time for me to retire I will receive a check at/near monthly equal amounts for the remainder of my life [till death.] The downside of this is that once it’s time for me to retire, and should markets perform higher hence provide higher returns, I will not reap those benefits. It’s one stable check market win or loose.\n\n\nCLICK TO VIEW CODE \n\n```r\n\ncalculate_benefit &lt;- function(FAS, N, CPI) {\n  \n  # Step 1: Calculate base benefit\n  if (N &lt;= 20) {\n    base_benefit &lt;- 1.67 / 100 * FAS * N\n  } else if (N == 20) {\n    base_benefit &lt;- 1.75 / 100 * FAS * N\n  } else {\n    base_benefit &lt;- (35 / 100 + 2 / 100 * (N - 20)) * FAS\n  }\n  \n  # Step 2: Calculate inflation adjustment\n  inflation_adjustment &lt;- 0.5 * CPI\n  inflation_adjustment &lt;- ceiling(inflation_adjustment * 10) / 10  # Round up to nearest tenth\n  \n  # Cap inflation adjustment between 1% and 3%\n  if (inflation_adjustment &lt; 1) {\n    inflation_adjustment &lt;- 1\n  } else if (inflation_adjustment &gt; 3) {\n    inflation_adjustment &lt;- 3\n  }\n  \n  # Step 3: Apply inflation adjustment to base benefit\n  adjusted_benefit &lt;- base_benefit * (1 + inflation_adjustment / 100)\n  \n  # Return the adjusted benefit\n  return(adjusted_benefit)\n}\n\n```\n\n\n# Example:\nFAS &lt;- 90000  # Example Final Average Salary\nN &lt;- 20       # Example number of years served\nCPI &lt;- 300    # Example CPI \n\nFor this example TRS benefit I used 90k as my final average salary. I can adjust accordingly on my R code. Here are my monthly Benefits for the TRS plan based on these metrics.\n\n\n[1] 2580.15\n\n\n\n\nORP\nI will now explore ORP monthly benefits. The ORP benefit is like a 401k, so if markets are performing well in my retirement then I potentially have greater monthly benefits or maybe longer benefits. I must keep in mind this is not forever [till death], so it’s important to invest well as well as to be cognizant of how much I will extract yearly for whatever number of years I think I will continue to live after retirement. So I guess one of the bigger downside is this plan will keep me alert on the number of years I will be alive - jaja. I also have a risk of markets not performing well. Unless of course I’m an excellent money manager and investor. In any case let’s look at the numbers. Here are my monthly benefits :\n\n\nCLICK TO VIEW CODE \n\n```r\n\n#CALCULATING ORP BENEFIT \n# Define the function to calculate total savings\ncalculate_orp_savings &lt;- function(initial_salary, salary_increase_rate, years_at_cuny, starting_age) {\n  \n  # Define the asset allocation and average returns for each period\n  asset_allocation &lt;- list(\n    age_25_49 = c(US_Equities = 0.54, International_Equities = 0.36, Bonds = 0.10),\n    age_50_59 = c(US_Equities = 0.47, International_Equities = 0.32, Bonds = 0.21),\n    age_60_74 = c(US_Equities = 0.34, International_Equities = 0.23, Bonds = 0.43),\n    age_75_plus = c(US_Equities = 0.19, International_Equities = 0.13, Bonds = 0.62, Short_Term_Debt = 0.06)\n  )\n  \n  returns &lt;- c(US_Equities = 0.07, International_Equities = 0.06, Bonds = 0.04, Short_Term_Debt = 0.02)\n  \n  # Function to compute contributions\n  calc_contributions &lt;- function(salary, years) {\n    contributions &lt;- numeric(years)\n    for (year in 1:years) {\n      if (salary &lt;= 45000) {\n        employee_contrib &lt;- 0.03 * salary\n      } else if (salary &lt;= 55000) {\n        employee_contrib &lt;- 0.035 * salary\n      } else if (salary &lt;= 75000) {\n        employee_contrib &lt;- 0.045 * salary\n      } else if (salary &lt;= 100000) {\n        employee_contrib &lt;- 0.0575 * salary\n      } else {\n        employee_contrib &lt;- 0.06 * salary\n      }\n      \n      if (year &lt;= 7) {\n        employer_contrib &lt;- 0.08 * salary\n      } else {\n        employer_contrib &lt;- 0.10 * salary\n      }\n      \n      total_contrib &lt;- employee_contrib + employer_contrib\n      contributions[year] &lt;- total_contrib\n      salary &lt;- salary * (1 + salary_increase_rate)  # Salary increases yearly\n    }\n    return(contributions)\n  }\n  \n  # Calculate contributions for all years\n  contributions &lt;- calc_contributions(initial_salary, years_at_cuny)\n  \n  # Calculate asset growth by age range\n  total_savings &lt;- 0\n  current_age &lt;- starting_age\n  \n  for (year in 1:years_at_cuny) {\n    # Determine the asset allocation based on age\n    if (current_age &gt;= 75) {\n      allocation &lt;- asset_allocation$age_75_plus\n    } else if (current_age &gt;= 60) {\n      allocation &lt;- asset_allocation$age_60_74\n    } else if (current_age &gt;= 50) {\n      allocation &lt;- asset_allocation$age_50_59\n    } else {\n      allocation &lt;- asset_allocation$age_25_49\n    }\n    \n    # Calculate the return for this year\n    growth_rate &lt;- sum(allocation * returns)\n    total_savings &lt;- total_savings * (1 + growth_rate) + contributions[year]\n    \n    # Increase the age by 1 year\n    current_age &lt;- current_age + 1\n  }\n  \n  return(total_savings)\n}\n\n```\n\nFor this ORP benefit example I am assuming the below scenario. I’m am also calculating 20 years of retirement\n\n# Example:\ninitial_salary &lt;- 65000\nsalary_increase_rate &lt;- 0.03  # 3% annual increase in salary\nyears_at_cuny &lt;- 20           # Employee works 20 years at CUNY\nstarting_age &lt;- 25 \n\n#NOTE: 20 years monthly benefits after retirement#\n\n\n\n[1] 2110.913\n\n\nWith both Calculations there are a ton of things to keep in mind. For TRS I’m assuming a 90K AVG salary at end of my tenure. It could be more or less. For ORP, I am assuming a 65K starting salary with monthly increase rate at 3%, but we must also consider market performance. Also could be more or less. So these numbers are just estimates.\n\n\n\nMONTE CARLO ANALYSIS\nSo I want to have more confidence of my future returns. I will now apply the Monte Carlo analysis to my calculations. Here are my results:\n\nTRS\n\n\nCLICK TO VIEW CODE \n\n```r\n\n### APPLY MONTE CARLO TO TRS \n### # Load necessary library\nlibrary(MASS)  # For simulating random variables (like FAS and CPI)\n\n# Function to calculate the retirement benefit with Monte Carlo Analysis\nmonte_carlo_benefit_trs &lt;- function(initial_FAS, mean_CPI, sd_CPI, mean_years_served, sd_years_served, num_simulations = 200) {\n  \n  # Function to calculate the benefit\n  calculate_benefit_trs &lt;- function(FAS, N, CPI) {\n    # Step 1: Calculate base benefit\n    if (N &lt;= 20) {\n      base_benefit &lt;- 1.67 / 100 * FAS * N\n    } else if (N == 20) {\n      base_benefit &lt;- 1.75 / 100 * FAS * N\n    } else {\n      base_benefit &lt;- (35 / 100 + 2 / 100 * (N - 20)) * FAS\n    }\n    \n    # Step 2: Calculate inflation adjustment\n    inflation_adjustment &lt;- 0.5 * CPI\n    inflation_adjustment &lt;- ceiling(inflation_adjustment * 10) / 10  # Round up to nearest tenth\n    \n    # Cap inflation adjustment between 1% and 3%\n    if (inflation_adjustment &lt; 1) {\n      inflation_adjustment &lt;- 1\n    } else if (inflation_adjustment &gt; 3) {\n      inflation_adjustment &lt;- 3\n    }\n    \n    # Step 3: Apply inflation adjustment to base benefit\n    adjusted_benefit &lt;- base_benefit * (1 + inflation_adjustment / 100)\n    \n    # Return the adjusted benefit\n    return(adjusted_benefit)\n  }\n  \n  # Monte Carlo Simulation\n  set.seed(123)  # Set seed for reproducibility\n  simulation_results_trs &lt;- numeric(num_simulations)\n  \n  for (sim in 1:num_simulations) {\n    # Simulate FAS (Final Average Salary) as a normal distribution\n    FAS &lt;- rnorm(1, mean = initial_FAS, sd = 5000)  # Mean FAS and standard deviation\n    \n    # Simulate Years of Service (N) as a normal distribution (or you can use uniform)\n    N &lt;- round(rnorm(1, mean = mean_years_served, sd = sd_years_served))\n    N &lt;- max(N, 1)  # Ensure at least 1 year of service\n    \n    # Simulate CPI (Consumer Price Index) as a normal distribution\n    CPI &lt;- rnorm(1, mean = mean_CPI, sd = sd_CPI)\n    \n    # Calculate the benefit for this simulation\n    benefit &lt;- calculate_benefit(FAS, N, CPI)\n    \n    # Store the result of this simulation\n    simulation_results_trs[sim] &lt;- benefit\n  }\n  \n  # Return simulation results (mean, standard deviation, and percentiles)\n  results_trs &lt;- list(\n    mean = mean(simulation_results_trs),\n    sd = sd(simulation_results_trs),\n    min = min(simulation_results_trs),\n    max = max(simulation_results_trs),\n    p10 = quantile(simulation_results_trs, 0.10),\n    p90 = quantile(simulation_results_trs, 0.90)\n  )\n  \n  return(results_trs)\n}\n\n# Example Usage of Monte Carlo Simulation\ninitial_FAS &lt;- 90000     # Mean Final Average Salary\nmean_CPI &lt;- 300          # Expected CPI (e.g., inflation rate)\nsd_CPI &lt;- 10             # Standard deviation for CPI\nmean_years_served &lt;- 20  # Mean number of years served\nsd_years_served &lt;- 2     # Standard deviation for years served\n\n# Run Monte Carlo Simulation with 10,000 iterations\nsimulation_trs_results &lt;- monte_carlo_benefit_trs(initial_FAS, mean_CPI, sd_CPI, mean_years_served, sd_years_served, num_simulations = 200)\n\n# Assuming simulation_trs_results contains summary statistics\nsimulation_trs_results &lt;- list(\n  mean = 31587.42,\n  sd = 4174.008,\n  min = 22687.68,\n  max = 42848.84,\n  p10 = 26792.82,\n  p90 = 37691.25\n)\n\n# Divide each value in the results by 240\nsimulation_trs_results_monthly &lt;- lapply(simulation_trs_results, function(x) x / 12)\n\n# Convert the results into a data frame for easy viewing\nsimulation_trs_results_table &lt;- data.frame(\n  Statistic = names(simulation_trs_results_monthly),\n  Monthly_Value = unlist(simulation_trs_results_monthly)\n)\n\n```\n\n\n\n     Statistic Monthly_Value\nmean      mean      2632.285\nsd          sd       347.834\nmin        min      1890.640\nmax        max      3570.737\np10        p10      2232.735\np90        p90      3140.938\n\n\n\n\nORP\n\n\nCLICK TO VIEW CODE \n\n```r\n\n###MONTE CARLO FOR ORP\n# Load necessary library\nlibrary(MASS)  # For simulating random variables (like salary increases and returns)\n\n# Define the function to calculate ORP savings with randomness (Monte Carlo Simulation)\nmonte_carlo_orp_savings &lt;- function(initial_salary, salary_increase_rate, years_at_cuny, starting_age, num_simulations = 200) {\n  \n  # Define the asset allocation and average returns for each period\n  asset_allocation &lt;- list(\n    age_25_49 = c(US_Equities = 0.54, International_Equities = 0.36, Bonds = 0.10),\n    age_50_59 = c(US_Equities = 0.47, International_Equities = 0.32, Bonds = 0.21),\n    age_60_74 = c(US_Equities = 0.34, International_Equities = 0.23, Bonds = 0.43),\n    age_75_plus = c(US_Equities = 0.19, International_Equities = 0.13, Bonds = 0.62, Short_Term_Debt = 0.06)\n  )\n  \n  returns &lt;- c(US_Equities = 0.07, International_Equities = 0.06, Bonds = 0.04, Short_Term_Debt = 0.02)\n  \n  # Function to compute contributions (same as before)\n  calc_contributions &lt;- function(salary, years) {\n    contributions &lt;- numeric(years)\n    for (year in 1:years) {\n      if (salary &lt;= 45000) {\n        employee_contrib &lt;- 0.03 * salary\n      } else if (salary &lt;= 55000) {\n        employee_contrib &lt;- 0.035 * salary\n      } else if (salary &lt;= 75000) {\n        employee_contrib &lt;- 0.045 * salary\n      } else if (salary &lt;= 100000) {\n        employee_contrib &lt;- 0.0575 * salary\n      } else {\n        employee_contrib &lt;- 0.06 * salary\n      }\n      \n      if (year &lt;= 7) {\n        employer_contrib &lt;- 0.08 * salary\n      } else {\n        employer_contrib &lt;- 0.10 * salary\n      }\n      \n      total_contrib &lt;- employee_contrib + employer_contrib\n      contributions[year] &lt;- total_contrib\n      salary &lt;- salary * (1 + salary_increase_rate)  # Salary increases yearly\n    }\n    return(contributions)\n  }\n  \n  # Monte Carlo Simulation\n  set.seed(123)  # Set seed for reproducibility\n  simulation_results &lt;- numeric(num_simulations)\n  \n  for (sim in 1:num_simulations) {\n    # Simulate the contribution and returns for each simulation\n    contributions &lt;- calc_contributions(initial_salary, years_at_cuny)\n    \n    # Simulate returns as random variables (e.g., normal distribution with mean returns and standard deviation)\n    total_savings &lt;- 0\n    current_age &lt;- starting_age\n    \n    for (year in 1:years_at_cuny) {\n      # Determine the asset allocation based on age\n      if (current_age &gt;= 75) {\n        allocation &lt;- asset_allocation$age_75_plus\n      } else if (current_age &gt;= 60) {\n        allocation &lt;- asset_allocation$age_60_74\n      } else if (current_age &gt;= 50) {\n        allocation &lt;- asset_allocation$age_50_59\n      } else {\n        allocation &lt;- asset_allocation$age_25_49\n      }\n      \n      # Simulate the growth rate by drawing random values for returns based on a normal distribution\n      simulated_growth_rate &lt;- sum(allocation * rnorm(length(returns), mean = returns, sd = 0.02))  # Small standard deviation for randomness\n      total_savings &lt;- total_savings * (1 + simulated_growth_rate) + contributions[year]\n      \n      # Increase the age by 1 year\n      current_age &lt;- current_age + 1\n    }\n    \n    # Store the result of this simulation\n    simulation_results[sim] &lt;- total_savings\n  }\n  \n  # Return simulation results (mean, standard deviation, and percentiles)\n  results &lt;- list(\n    mean = mean(simulation_results),\n    sd = sd(simulation_results),\n    min = min(simulation_results),\n    max = max(simulation_results),\n    p10 = quantile(simulation_results, 0.10),\n    p90 = quantile(simulation_results, 0.90)\n  )\n  \n  return(results)\n}\n\n# Example Usage of Monte Carlo Simulation\ninitial_salary &lt;- 65000\nsalary_increase_rate &lt;- 0.03  # 3% annual increase in salary\nyears_at_cuny &lt;- 20           # Employee works 20 years at CUNY\nstarting_age &lt;- 25            # Start at age 25\n\n# Run Monte Carlo Simulation with 10,000 iterations\nsimulation_results &lt;- monte_carlo_orp_savings(initial_salary, salary_increase_rate, years_at_cuny, starting_age, num_simulations = 200)\n\n# Display the results\nview(simulation_results)\n\n# Assuming simulation_results contains summary statistics\nsimulation_results &lt;- list(\n  mean = 505804.1,\n  sd = 19940.14,\n  min = 450809.8,\n  max = 571327.3,\n  p10 = 482795.2,\n  p90 = 531700.4\n)\n\n# Convert total results to monthly results by dividing by 240 months\nsimulation_results_monthly &lt;- lapply(simulation_results, function(x) x / 240)\n\n# Convert the list into a data frame for easy viewing\nsimulation_results_table &lt;- data.frame(\n  Statistic = names(simulation_results_monthly),\n  Monthly_Value = unlist(simulation_results_monthly)\n)\n\n```\n\n\n\n     Statistic Monthly_Value\nmean      mean    2107.51708\nsd          sd      83.08392\nmin        min    1878.37417\nmax        max    2380.53042\np10        p10    2011.64667\np90        p90    2215.41833\n\n\nRegardless of the outcomes and end benefits for both. I have already made a decision based on economic analysis from FRED and ALPHA historical data. Nothing is certain is my conclusion, and since I’m not looking to be rich but rather just live comfortably enjoying time with family, frugally, I will choose the TRS benefit; the benefit for life. Perhaps on the side saving/investing for my health bills will do good, unless of course healthcare becomes FREE - then in that case I have nothing more to say that WONDERFUL!!\n\n\n\nRETIREMENT HERE I COME"
  }
]